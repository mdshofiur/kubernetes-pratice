* 
* ==> Audit <==
* |------------|-----------------------|----------|----------------------|---------|---------------------|---------------------|
|  Command   |         Args          | Profile  |         User         | Version |     Start Time      |      End Time       |
|------------|-----------------------|----------|----------------------|---------|---------------------|---------------------|
| config     | set driver docker     | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 05 Nov 23 16:36 +06 | 05 Nov 23 16:36 +06 |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 05 Nov 23 16:36 +06 | 05 Nov 23 16:37 +06 |
| dashboard  |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 05 Nov 23 16:38 +06 |                     |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 05 Nov 23 16:38 +06 | 05 Nov 23 16:38 +06 |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 14:56 +06 |                     |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 15:02 +06 | 06 Nov 23 15:04 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 15:46 +06 | 06 Nov 23 15:49 +06 |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 15:49 +06 | 06 Nov 23 15:49 +06 |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:08 +06 | 06 Nov 23 21:09 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:10 +06 | 06 Nov 23 21:12 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:14 +06 | 06 Nov 23 21:22 +06 |
| docker-env |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:22 +06 | 06 Nov 23 21:22 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:23 +06 | 06 Nov 23 21:29 +06 |
| config     | set driver docker     | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:29 +06 | 06 Nov 23 21:29 +06 |
| start      | --driver=docker       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:29 +06 | 06 Nov 23 21:30 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:31 +06 | 06 Nov 23 21:43 +06 |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 06 Nov 23 21:43 +06 | 06 Nov 23 21:43 +06 |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 21:46 +06 | 07 Nov 23 21:49 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 21:51 +06 | 07 Nov 23 22:06 +06 |
| ip         |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 21:55 +06 | 07 Nov 23 21:55 +06 |
| ip         |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 21:57 +06 | 07 Nov 23 21:57 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 22:10 +06 | 07 Nov 23 22:12 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 22:23 +06 | 07 Nov 23 22:26 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 22:31 +06 | 07 Nov 23 22:31 +06 |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 22:31 +06 | 07 Nov 23 22:32 +06 |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 22:32 +06 | 07 Nov 23 22:33 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 22:35 +06 | 07 Nov 23 22:35 +06 |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 07 Nov 23 22:36 +06 | 07 Nov 23 22:36 +06 |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 18:30 +06 |                     |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 18:33 +06 | 08 Nov 23 18:34 +06 |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 18:56 +06 | 08 Nov 23 18:56 +06 |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 19:45 +06 | 08 Nov 23 19:47 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 19:53 +06 | 08 Nov 23 20:01 +06 |
| service    | nginx-service --url   | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 20:01 +06 | 08 Nov 23 20:01 +06 |
| service    | nginx-service --url   | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 20:01 +06 | 08 Nov 23 20:02 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 20:02 +06 | 08 Nov 23 20:03 +06 |
| tunnel     | --cleanup             | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 20:03 +06 | 08 Nov 23 20:04 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 20:09 +06 | 08 Nov 23 20:12 +06 |
| ip         |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 20:11 +06 | 08 Nov 23 20:11 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 20:12 +06 | 08 Nov 23 20:15 +06 |
| service    | nginx-service --url   | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 20:14 +06 | 08 Nov 23 21:32 +06 |
| service    | backend-service --url | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 21:50 +06 | 08 Nov 23 21:51 +06 |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 08 Nov 23 22:10 +06 | 08 Nov 23 22:11 +06 |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 10:42 +06 | 09 Nov 23 10:44 +06 |
| service    | backend-service --url | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 11:00 +06 | 09 Nov 23 11:05 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 11:05 +06 |                     |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 13:29 +06 | 09 Nov 23 13:30 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 13:48 +06 | 09 Nov 23 13:49 +06 |
| service    | backend-service --url | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 13:50 +06 | 09 Nov 23 13:50 +06 |
| tunnel     |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 13:51 +06 | 09 Nov 23 13:56 +06 |
| addons     | enable ingress        | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 13:55 +06 |                     |
| addons     | enable ingress        | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 13:55 +06 |                     |
| addons     | enable ingress        | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 13:57 +06 |                     |
| addons     | enable ingress        | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 14:04 +06 |                     |
| service    | backend-service --url | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 14:37 +06 | 09 Nov 23 14:38 +06 |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 14:38 +06 | 09 Nov 23 14:38 +06 |
| stop       |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 14:42 +06 |                     |
| start      |                       | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 14:42 +06 | 09 Nov 23 14:43 +06 |
| addons     | enable ingress        | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 16:23 +06 |                     |
| addons     | enable ingress        | minikube | DESKTOP-UO7I6LP\DELL | v1.31.2 | 09 Nov 23 16:24 +06 |                     |
|------------|-----------------------|----------|----------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/11/09 14:42:16
Running on machine: DESKTOP-UO7I6LP
Binary: Built with gc go1.20.7 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1109 14:42:16.437857   14400 out.go:296] Setting OutFile to fd 88 ...
I1109 14:42:16.438955   14400 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1109 14:42:16.438955   14400 out.go:309] Setting ErrFile to fd 92...
I1109 14:42:16.438955   14400 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1109 14:42:16.458106   14400 out.go:303] Setting JSON to false
I1109 14:42:16.464035   14400 start.go:128] hostinfo: {"hostname":"DESKTOP-UO7I6LP","uptime":258190,"bootTime":1699261146,"procs":264,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.3570 Build 19045.3570","kernelVersion":"10.0.19045.3570 Build 19045.3570","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"af9f9034-61aa-4784-8247-604b10328b0b"}
W1109 14:42:16.464035   14400 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1109 14:42:16.465663   14400 out.go:177] * minikube v1.31.2 on Microsoft Windows 10 Pro 10.0.19045.3570 Build 19045.3570
I1109 14:42:16.468901   14400 notify.go:220] Checking for updates...
I1109 14:42:16.469434   14400 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1109 14:42:16.473766   14400 driver.go:373] Setting default libvirt URI to qemu:///system
I1109 14:42:16.692666   14400 docker.go:121] docker version: linux-24.0.6:Docker Desktop 4.24.2 (124339)
I1109 14:42:16.699888   14400 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1109 14:42:20.698595   14400 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.9985994s)
I1109 14:42:20.701818   14400 info.go:266] docker info: {ID:dd496783-3e8f-45c9-bc14-d5c16b87d28d Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:46 OomKillDisable:true NGoroutines:71 SystemTime:2023-11-09 08:42:20.628146509 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4060893184 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1109 14:42:20.704487   14400 out.go:177] * Using the docker driver based on existing profile
I1109 14:42:20.705566   14400 start.go:298] selected driver: docker
I1109 14:42:20.705566   14400 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\DELL:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1109 14:42:20.705566   14400 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1109 14:42:20.722111   14400 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1109 14:42:21.484641   14400 info.go:266] docker info: {ID:dd496783-3e8f-45c9-bc14-d5c16b87d28d Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:46 OomKillDisable:true NGoroutines:71 SystemTime:2023-11-09 08:42:21.423062983 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4060893184 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1109 14:42:21.620689   14400 cni.go:84] Creating CNI manager for ""
I1109 14:42:21.620689   14400 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1109 14:42:21.620689   14400 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\DELL:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1109 14:42:21.622818   14400 out.go:177] * Starting control plane node minikube in cluster minikube
I1109 14:42:21.623996   14400 cache.go:122] Beginning downloading kic base image for docker with docker
I1109 14:42:21.624644   14400 out.go:177] * Pulling base image ...
I1109 14:42:21.626233   14400 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1109 14:42:21.626233   14400 image.go:79] Checking for docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I1109 14:42:21.626771   14400 preload.go:148] Found local preload: C:\Users\DELL\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1109 14:42:21.626771   14400 cache.go:57] Caching tarball of preloaded images
I1109 14:42:21.627303   14400 preload.go:174] Found C:\Users\DELL\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1109 14:42:21.627303   14400 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I1109 14:42:21.627831   14400 profile.go:148] Saving config to C:\Users\DELL\.minikube\profiles\minikube\config.json ...
I1109 14:42:21.862106   14400 image.go:83] Found docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I1109 14:42:21.862106   14400 cache.go:145] docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I1109 14:42:21.862106   14400 cache.go:195] Successfully downloaded all kic artifacts
I1109 14:42:21.862646   14400 start.go:365] acquiring machines lock for minikube: {Name:mkec786be7b8f8d7d72f4c3ca5ce7520987e23b4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1109 14:42:21.862646   14400 start.go:369] acquired machines lock for "minikube" in 0s
I1109 14:42:21.863178   14400 start.go:96] Skipping create...Using existing machine configuration
I1109 14:42:21.863889   14400 fix.go:54] fixHost starting: 
I1109 14:42:21.880730   14400 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1109 14:42:22.077494   14400 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1109 14:42:22.077494   14400 fix.go:128] unexpected machine state, will restart: <nil>
I1109 14:42:22.078558   14400 out.go:177] * Restarting existing docker container for "minikube" ...
I1109 14:42:22.087831   14400 cli_runner.go:164] Run: docker start minikube
I1109 14:42:23.927488   14400 cli_runner.go:217] Completed: docker start minikube: (1.8396574s)
I1109 14:42:23.945820   14400 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1109 14:42:24.188441   14400 kic.go:426] container "minikube" state is running.
I1109 14:42:24.216958   14400 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1109 14:42:24.408612   14400 profile.go:148] Saving config to C:\Users\DELL\.minikube\profiles\minikube\config.json ...
I1109 14:42:24.412468   14400 machine.go:88] provisioning docker machine ...
I1109 14:42:24.428179   14400 ubuntu.go:169] provisioning hostname "minikube"
I1109 14:42:24.441984   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:24.643382   14400 main.go:141] libmachine: Using SSH client type: native
I1109 14:42:24.644517   14400 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xad9400] 0xadc2a0 <nil>  [] 0s} 127.0.0.1 58240 <nil> <nil>}
I1109 14:42:24.644517   14400 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1109 14:42:24.725798   14400 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1109 14:42:28.056286   14400 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1109 14:42:28.073334   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:28.314144   14400 main.go:141] libmachine: Using SSH client type: native
I1109 14:42:28.316908   14400 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xad9400] 0xadc2a0 <nil>  [] 0s} 127.0.0.1 58240 <nil> <nil>}
I1109 14:42:28.316908   14400 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1109 14:42:28.523074   14400 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1109 14:42:28.523624   14400 ubuntu.go:175] set auth options {CertDir:C:\Users\DELL\.minikube CaCertPath:C:\Users\DELL\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\DELL\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\DELL\.minikube\machines\server.pem ServerKeyPath:C:\Users\DELL\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\DELL\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\DELL\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\DELL\.minikube}
I1109 14:42:28.523624   14400 ubuntu.go:177] setting up certificates
I1109 14:42:28.523624   14400 provision.go:83] configureAuth start
I1109 14:42:28.531318   14400 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1109 14:42:28.699654   14400 provision.go:138] copyHostCerts
I1109 14:42:28.710310   14400 exec_runner.go:144] found C:\Users\DELL\.minikube/cert.pem, removing ...
I1109 14:42:28.710411   14400 exec_runner.go:203] rm: C:\Users\DELL\.minikube\cert.pem
I1109 14:42:28.710411   14400 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\cert.pem --> C:\Users\DELL\.minikube/cert.pem (1115 bytes)
I1109 14:42:28.723376   14400 exec_runner.go:144] found C:\Users\DELL\.minikube/key.pem, removing ...
I1109 14:42:28.723376   14400 exec_runner.go:203] rm: C:\Users\DELL\.minikube\key.pem
I1109 14:42:28.723911   14400 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\key.pem --> C:\Users\DELL\.minikube/key.pem (1679 bytes)
I1109 14:42:28.735639   14400 exec_runner.go:144] found C:\Users\DELL\.minikube/ca.pem, removing ...
I1109 14:42:28.735639   14400 exec_runner.go:203] rm: C:\Users\DELL\.minikube\ca.pem
I1109 14:42:28.736178   14400 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\ca.pem --> C:\Users\DELL\.minikube/ca.pem (1070 bytes)
I1109 14:42:28.737308   14400 provision.go:112] generating server cert: C:\Users\DELL\.minikube\machines\server.pem ca-key=C:\Users\DELL\.minikube\certs\ca.pem private-key=C:\Users\DELL\.minikube\certs\ca-key.pem org=DELL.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1109 14:42:29.108749   14400 provision.go:172] copyRemoteCerts
I1109 14:42:29.124875   14400 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1109 14:42:29.131092   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:29.329836   14400 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58240 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I1109 14:42:29.489802   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I1109 14:42:29.590076   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I1109 14:42:29.677030   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1109 14:42:29.763062   14400 provision.go:86] duration metric: configureAuth took 1.2389196s
I1109 14:42:29.763062   14400 ubuntu.go:193] setting minikube options for container-runtime
I1109 14:42:29.763381   14400 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1109 14:42:29.774837   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:29.963547   14400 main.go:141] libmachine: Using SSH client type: native
I1109 14:42:29.964626   14400 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xad9400] 0xadc2a0 <nil>  [] 0s} 127.0.0.1 58240 <nil> <nil>}
I1109 14:42:29.964626   14400 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1109 14:42:30.157596   14400 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1109 14:42:30.157596   14400 ubuntu.go:71] root file system type: overlay
I1109 14:42:30.158160   14400 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1109 14:42:30.170097   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:30.355321   14400 main.go:141] libmachine: Using SSH client type: native
I1109 14:42:30.356410   14400 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xad9400] 0xadc2a0 <nil>  [] 0s} 127.0.0.1 58240 <nil> <nil>}
I1109 14:42:30.356951   14400 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1109 14:42:30.576340   14400 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1109 14:42:30.588439   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:30.768799   14400 main.go:141] libmachine: Using SSH client type: native
I1109 14:42:30.769664   14400 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xad9400] 0xadc2a0 <nil>  [] 0s} 127.0.0.1 58240 <nil> <nil>}
I1109 14:42:30.769664   14400 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1109 14:42:30.987422   14400 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1109 14:42:30.987422   14400 machine.go:91] provisioned docker machine in 6.5749537s
I1109 14:42:30.987422   14400 start.go:300] post-start starting for "minikube" (driver="docker")
I1109 14:42:30.987663   14400 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1109 14:42:31.003864   14400 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1109 14:42:31.011169   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:31.194580   14400 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58240 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I1109 14:42:31.356163   14400 ssh_runner.go:195] Run: cat /etc/os-release
I1109 14:42:31.364569   14400 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1109 14:42:31.364569   14400 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1109 14:42:31.364569   14400 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1109 14:42:31.364569   14400 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I1109 14:42:31.365084   14400 filesync.go:126] Scanning C:\Users\DELL\.minikube\addons for local assets ...
I1109 14:42:31.369849   14400 filesync.go:126] Scanning C:\Users\DELL\.minikube\files for local assets ...
I1109 14:42:31.370391   14400 start.go:303] post-start completed in 382.9697ms
I1109 14:42:31.381765   14400 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1109 14:42:31.390611   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:31.579457   14400 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58240 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I1109 14:42:31.708348   14400 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1109 14:42:31.719726   14400 fix.go:56] fixHost completed within 9.8560399s
I1109 14:42:31.719726   14400 start.go:83] releasing machines lock for "minikube", held for 9.8570803s
I1109 14:42:31.726708   14400 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1109 14:42:31.913419   14400 ssh_runner.go:195] Run: cat /version.json
I1109 14:42:31.915051   14400 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1109 14:42:31.925221   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:31.925221   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:42:32.133625   14400 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58240 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I1109 14:42:32.156391   14400 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58240 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I1109 14:42:33.404854   14400 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.4898022s)
I1109 14:42:33.404854   14400 ssh_runner.go:235] Completed: cat /version.json: (1.4914344s)
I1109 14:42:33.420441   14400 ssh_runner.go:195] Run: systemctl --version
I1109 14:42:33.445523   14400 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1109 14:42:33.473828   14400 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1109 14:42:33.525872   14400 start.go:410] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1109 14:42:33.543590   14400 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1109 14:42:33.571798   14400 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1109 14:42:33.571798   14400 start.go:466] detecting cgroup driver to use...
I1109 14:42:33.571798   14400 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1109 14:42:33.574039   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1109 14:42:33.647847   14400 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1109 14:42:33.696922   14400 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1109 14:42:33.728643   14400 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1109 14:42:33.742838   14400 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1109 14:42:33.785787   14400 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1109 14:42:33.829822   14400 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1109 14:42:33.872835   14400 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1109 14:42:33.915778   14400 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1109 14:42:33.958749   14400 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1109 14:42:34.002541   14400 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1109 14:42:34.044868   14400 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1109 14:42:34.087480   14400 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1109 14:42:34.287320   14400 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1109 14:42:34.512172   14400 start.go:466] detecting cgroup driver to use...
I1109 14:42:34.512172   14400 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1109 14:42:34.536667   14400 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1109 14:42:34.640093   14400 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1109 14:42:34.672691   14400 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1109 14:42:34.728178   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1109 14:42:34.844305   14400 ssh_runner.go:195] Run: which cri-dockerd
I1109 14:42:34.886202   14400 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1109 14:42:34.954754   14400 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1109 14:42:35.107536   14400 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1109 14:42:35.424018   14400 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1109 14:42:35.612932   14400 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I1109 14:42:35.612932   14400 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I1109 14:42:35.671277   14400 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1109 14:42:35.872091   14400 ssh_runner.go:195] Run: sudo systemctl restart docker
I1109 14:42:36.529871   14400 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1109 14:42:36.697207   14400 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1109 14:42:36.889415   14400 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1109 14:42:37.045845   14400 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1109 14:42:37.225294   14400 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1109 14:42:37.285449   14400 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1109 14:42:37.445307   14400 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1109 14:42:37.828073   14400 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1109 14:42:37.841476   14400 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1109 14:42:37.850253   14400 start.go:534] Will wait 60s for crictl version
I1109 14:42:37.861126   14400 ssh_runner.go:195] Run: which crictl
I1109 14:42:37.879801   14400 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1109 14:42:38.265136   14400 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I1109 14:42:38.272321   14400 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1109 14:42:38.330675   14400 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1109 14:42:38.377658   14400 out.go:204] * Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I1109 14:42:38.387333   14400 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1109 14:42:38.824668   14400 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1109 14:42:38.835356   14400 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1109 14:42:38.844559   14400 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1109 14:42:38.888674   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1109 14:42:39.059934   14400 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1109 14:42:39.067390   14400 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1109 14:42:39.108805   14400 docker.go:636] Got preloaded images: -- stdout --
shafikur/backend-one:2.0
shafikur/backend-one:1.0
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I1109 14:42:39.108805   14400 docker.go:566] Images already preloaded, skipping extraction
I1109 14:42:39.117136   14400 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1109 14:42:39.151027   14400 docker.go:636] Got preloaded images: -- stdout --
shafikur/backend-one:2.0
shafikur/backend-one:1.0
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I1109 14:42:39.151027   14400 cache_images.go:84] Images are preloaded, skipping loading
I1109 14:42:39.158430   14400 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1109 14:42:39.670353   14400 cni.go:84] Creating CNI manager for ""
I1109 14:42:39.670389   14400 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1109 14:42:39.671562   14400 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1109 14:42:39.671562   14400 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1109 14:42:39.672113   14400 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1109 14:42:39.672692   14400 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1109 14:42:39.689865   14400 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I1109 14:42:39.730906   14400 binaries.go:44] Found k8s binaries, skipping transfer
I1109 14:42:39.745366   14400 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1109 14:42:39.783569   14400 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1109 14:42:39.845256   14400 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1109 14:42:39.905537   14400 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1109 14:42:39.994943   14400 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1109 14:42:40.003841   14400 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1109 14:42:40.033312   14400 certs.go:56] Setting up C:\Users\DELL\.minikube\profiles\minikube for IP: 192.168.49.2
I1109 14:42:40.034318   14400 certs.go:190] acquiring lock for shared ca certs: {Name:mk065245d010ad3574bf79bb752816116d12e040 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1109 14:42:40.044605   14400 certs.go:199] skipping minikubeCA CA generation: C:\Users\DELL\.minikube\ca.key
I1109 14:42:40.062572   14400 certs.go:199] skipping proxyClientCA CA generation: C:\Users\DELL\.minikube\proxy-client-ca.key
I1109 14:42:40.063111   14400 certs.go:315] skipping minikube-user signed cert generation: C:\Users\DELL\.minikube\profiles\minikube\client.key
I1109 14:42:40.084766   14400 certs.go:315] skipping minikube signed cert generation: C:\Users\DELL\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1109 14:42:40.105569   14400 certs.go:315] skipping aggregator signed cert generation: C:\Users\DELL\.minikube\profiles\minikube\proxy-client.key
I1109 14:42:40.110483   14400 certs.go:437] found cert: C:\Users\DELL\.minikube\certs\C:\Users\DELL\.minikube\certs\ca-key.pem (1675 bytes)
I1109 14:42:40.110988   14400 certs.go:437] found cert: C:\Users\DELL\.minikube\certs\C:\Users\DELL\.minikube\certs\ca.pem (1070 bytes)
I1109 14:42:40.111018   14400 certs.go:437] found cert: C:\Users\DELL\.minikube\certs\C:\Users\DELL\.minikube\certs\cert.pem (1115 bytes)
I1109 14:42:40.111018   14400 certs.go:437] found cert: C:\Users\DELL\.minikube\certs\C:\Users\DELL\.minikube\certs\key.pem (1679 bytes)
I1109 14:42:40.112964   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1109 14:42:40.191650   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1109 14:42:40.272915   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1109 14:42:40.355125   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1109 14:42:40.430100   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1109 14:42:40.509156   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1109 14:42:40.600876   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1109 14:42:40.681729   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1109 14:42:40.745961   14400 ssh_runner.go:362] scp C:\Users\DELL\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1109 14:42:40.809808   14400 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1109 14:42:40.871429   14400 ssh_runner.go:195] Run: openssl version
I1109 14:42:40.904575   14400 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1109 14:42:40.947290   14400 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1109 14:42:40.955808   14400 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov  5 09:37 /usr/share/ca-certificates/minikubeCA.pem
I1109 14:42:40.967597   14400 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1109 14:42:40.995926   14400 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1109 14:42:41.041404   14400 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1109 14:42:41.067180   14400 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1109 14:42:41.093769   14400 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1109 14:42:41.122635   14400 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1109 14:42:41.151228   14400 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1109 14:42:41.177226   14400 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1109 14:42:41.202914   14400 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1109 14:42:41.217293   14400 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\DELL:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1109 14:42:41.222798   14400 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1109 14:42:41.268252   14400 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1109 14:42:41.295309   14400 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1109 14:42:41.295939   14400 kubeadm.go:636] restartCluster start
I1109 14:42:41.309317   14400 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1109 14:42:41.339365   14400 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1109 14:42:41.348187   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1109 14:42:41.551309   14400 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\DELL\.kube\config
I1109 14:42:41.551309   14400 kubeconfig.go:146] "minikube" context is missing from C:\Users\DELL\.kube\config - will repair!
I1109 14:42:41.551837   14400 lock.go:35] WriteFile acquiring C:\Users\DELL\.kube\config: {Name:mkebc657628023f432578f979d2189a052d2709f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1109 14:42:41.577319   14400 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1109 14:42:41.604289   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:41.616998   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:41.644210   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:41.644210   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:41.656392   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:41.683784   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:42.191755   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:42.201505   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:42.228645   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:42.689560   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:42.706225   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:42.741243   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:43.191559   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:43.223354   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:43.256755   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:43.686588   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:43.723810   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:43.756777   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:44.196140   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:44.226436   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:44.267302   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:44.698685   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:44.738821   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:44.779708   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:45.194110   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:45.228411   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:45.265957   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:45.698265   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:45.728226   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:45.754994   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:46.190318   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:46.230812   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:46.276416   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:46.694854   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:46.713716   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:46.742624   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:47.194455   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:47.232423   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:47.273638   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:47.695318   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:47.726861   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:47.772932   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:48.186015   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:48.221943   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:48.253521   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:48.693890   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:48.727215   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:48.771688   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:49.196067   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:49.207480   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:49.238315   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:49.698138   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:49.719833   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:49.751906   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:50.195929   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:50.234417   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:50.288519   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:50.698162   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:50.731485   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:50.777865   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:51.195174   14400 api_server.go:166] Checking apiserver status ...
I1109 14:42:51.221420   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1109 14:42:51.249660   14400 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1109 14:42:51.616336   14400 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1109 14:42:51.616526   14400 kubeadm.go:1128] stopping kube-system containers ...
I1109 14:42:51.626196   14400 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1109 14:42:51.674642   14400 docker.go:462] Stopping containers: [347e16c04d5e 8fe1dc783679 b666da8b9df2 499fad4f9858 c35f5c07afe3 5e8b09b6d1b6 cbe956342d16 a85d3b7f88a3 5ff8aacb1474 5b1a4d3f7038 a496aa10b36d d0d6d43d5087 bd918011244a b8126cbcb71b 60b4b0c08739 ff50a48ffcdf 1a330e944b0e a6509b456df7 2bd197097818 281a808a0ec8 98f821881961 7e15d05b266c 62b8743bcf4b a147a3aac698 e7214cfbd06a 77dac571d558 bc14e82c179e]
I1109 14:42:51.684770   14400 ssh_runner.go:195] Run: docker stop 347e16c04d5e 8fe1dc783679 b666da8b9df2 499fad4f9858 c35f5c07afe3 5e8b09b6d1b6 cbe956342d16 a85d3b7f88a3 5ff8aacb1474 5b1a4d3f7038 a496aa10b36d d0d6d43d5087 bd918011244a b8126cbcb71b 60b4b0c08739 ff50a48ffcdf 1a330e944b0e a6509b456df7 2bd197097818 281a808a0ec8 98f821881961 7e15d05b266c 62b8743bcf4b a147a3aac698 e7214cfbd06a 77dac571d558 bc14e82c179e
I1109 14:42:51.767222   14400 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1109 14:42:51.839533   14400 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1109 14:42:51.894112   14400 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Nov  5 09:37 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Nov  9 07:30 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Nov  5 09:38 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Nov  9 07:30 /etc/kubernetes/scheduler.conf

I1109 14:42:51.932737   14400 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1109 14:42:51.999096   14400 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1109 14:42:52.048137   14400 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1109 14:42:52.079175   14400 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1109 14:42:52.093306   14400 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1109 14:42:52.147870   14400 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1109 14:42:52.221143   14400 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1109 14:42:52.243444   14400 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1109 14:42:52.312766   14400 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1109 14:42:52.369977   14400 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1109 14:42:52.369977   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1109 14:42:53.039082   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1109 14:42:54.849197   14400 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.8101153s)
I1109 14:42:54.849197   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1109 14:42:55.117184   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1109 14:42:55.229270   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1109 14:42:55.325789   14400 api_server.go:52] waiting for apiserver process to appear ...
I1109 14:42:55.340599   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:42:55.389858   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:42:55.950702   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:42:56.453678   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:42:56.945715   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:42:57.446349   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:42:57.945248   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:42:58.447038   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:42:58.605510   14400 api_server.go:72] duration metric: took 3.2797204s to wait for apiserver process to appear ...
I1109 14:42:58.605510   14400 api_server.go:88] waiting for apiserver healthz status ...
I1109 14:42:58.605510   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:42:58.609526   14400 api_server.go:269] stopped: https://127.0.0.1:58239/healthz: Get "https://127.0.0.1:58239/healthz": EOF
I1109 14:42:58.609526   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:42:58.615263   14400 api_server.go:269] stopped: https://127.0.0.1:58239/healthz: Get "https://127.0.0.1:58239/healthz": EOF
I1109 14:42:59.117643   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:42:59.123076   14400 api_server.go:269] stopped: https://127.0.0.1:58239/healthz: Get "https://127.0.0.1:58239/healthz": EOF
I1109 14:42:59.625224   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:42:59.628752   14400 api_server.go:269] stopped: https://127.0.0.1:58239/healthz: Get "https://127.0.0.1:58239/healthz": EOF
I1109 14:43:00.116937   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:04.720425   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1109 14:43:04.720425   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1109 14:43:04.720425   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:04.900886   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1109 14:43:04.900903   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1109 14:43:05.116607   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:05.191996   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1109 14:43:05.191996   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1109 14:43:05.627368   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:05.641981   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1109 14:43:05.641981   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1109 14:43:06.121291   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:06.196633   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1109 14:43:06.196633   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1109 14:43:06.636317   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:06.730615   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1109 14:43:06.730615   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1109 14:43:07.118478   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:07.296931   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1109 14:43:07.296931   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1109 14:43:07.619116   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:07.703554   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1109 14:43:07.704089   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1109 14:43:08.136495   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:08.270135   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1109 14:43:08.270135   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1109 14:43:08.620573   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:08.639546   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1109 14:43:08.639546   14400 api_server.go:103] status: https://127.0.0.1:58239/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1109 14:43:09.137722   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:09.318921   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 200:
ok
I1109 14:43:09.401548   14400 api_server.go:141] control plane version: v1.27.4
I1109 14:43:09.401548   14400 api_server.go:131] duration metric: took 10.7960381s to wait for apiserver health ...
I1109 14:43:09.401548   14400 cni.go:84] Creating CNI manager for ""
I1109 14:43:09.401548   14400 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1109 14:43:09.406532   14400 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I1109 14:43:09.457060   14400 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1109 14:43:09.633657   14400 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1109 14:43:09.957449   14400 system_pods.go:43] waiting for kube-system pods to appear ...
I1109 14:43:10.158969   14400 system_pods.go:59] 7 kube-system pods found
I1109 14:43:10.159008   14400 system_pods.go:61] "coredns-5d78c9869d-j65kd" [6fdba634-e05e-494d-bafc-cb3f69f0daa2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1109 14:43:10.159008   14400 system_pods.go:61] "etcd-minikube" [372d77a0-3e3e-4b30-ae2b-f1feed674024] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1109 14:43:10.159008   14400 system_pods.go:61] "kube-apiserver-minikube" [8587dfbf-7f7e-4458-bab9-6c418a61af46] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1109 14:43:10.159008   14400 system_pods.go:61] "kube-controller-manager-minikube" [614a6e6f-63e2-45f4-a484-59432267ccfc] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1109 14:43:10.159008   14400 system_pods.go:61] "kube-proxy-km2wn" [898207e3-ac85-4c73-81eb-4f066dde7965] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1109 14:43:10.159008   14400 system_pods.go:61] "kube-scheduler-minikube" [a7fd0196-78d6-4e84-9180-e2f47cb657d9] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1109 14:43:10.159008   14400 system_pods.go:61] "storage-provisioner" [714474ba-8971-4f5e-aed4-f51cad11fb15] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1109 14:43:10.159008   14400 system_pods.go:74] duration metric: took 201.5591ms to wait for pod list to return data ...
I1109 14:43:10.159008   14400 node_conditions.go:102] verifying NodePressure condition ...
I1109 14:43:10.274456   14400 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1109 14:43:10.274456   14400 node_conditions.go:123] node cpu capacity is 4
I1109 14:43:10.284847   14400 node_conditions.go:105] duration metric: took 125.8384ms to run NodePressure ...
I1109 14:43:10.287893   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1109 14:43:14.020076   14400 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (3.7321833s)
I1109 14:43:14.020076   14400 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1109 14:43:14.361234   14400 ops.go:34] apiserver oom_adj: -16
I1109 14:43:14.361234   14400 kubeadm.go:640] restartCluster took 33.0652948s
I1109 14:43:14.361234   14400 kubeadm.go:406] StartCluster complete in 33.1439412s
I1109 14:43:14.364463   14400 settings.go:142] acquiring lock: {Name:mkdce26dab5b02003dd3dd1b089da3b89ebae416 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1109 14:43:14.366946   14400 settings.go:150] Updating kubeconfig:  C:\Users\DELL\.kube\config
I1109 14:43:14.372332   14400 lock.go:35] WriteFile acquiring C:\Users\DELL\.kube\config: {Name:mkebc657628023f432578f979d2189a052d2709f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1109 14:43:14.386826   14400 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1109 14:43:14.418799   14400 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1109 14:43:14.402823   14400 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1109 14:43:14.419506   14400 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1109 14:43:14.419506   14400 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W1109 14:43:14.419506   14400 addons.go:240] addon storage-provisioner should already be in state true
I1109 14:43:14.420034   14400 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1109 14:43:14.420034   14400 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1109 14:43:14.420034   14400 addons.go:69] Setting dashboard=true in profile "minikube"
I1109 14:43:14.420034   14400 addons.go:231] Setting addon dashboard=true in "minikube"
W1109 14:43:14.424038   14400 addons.go:240] addon dashboard should already be in state true
I1109 14:43:14.427305   14400 host.go:66] Checking if "minikube" exists ...
I1109 14:43:14.429228   14400 host.go:66] Checking if "minikube" exists ...
I1109 14:43:14.458170   14400 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1109 14:43:14.474230   14400 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1109 14:43:14.476255   14400 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1109 14:43:15.342286   14400 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1109 14:43:15.346914   14400 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I1109 14:43:15.347908   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1109 14:43:15.347908   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1109 14:43:15.357908   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:43:15.366616   14400 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1109 14:43:15.370516   14400 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1109 14:43:15.370516   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1109 14:43:15.385308   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:43:15.538888   14400 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1109 14:43:15.539989   14400 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1109 14:43:15.540788   14400 out.go:177] * Verifying Kubernetes components...
I1109 14:43:15.564945   14400 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1109 14:43:15.649163   14400 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1109 14:43:15.649163   14400 addons.go:240] addon default-storageclass should already be in state true
I1109 14:43:15.649746   14400 host.go:66] Checking if "minikube" exists ...
I1109 14:43:15.685229   14400 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1109 14:43:15.732934   14400 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58240 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I1109 14:43:15.739559   14400 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58240 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I1109 14:43:15.976171   14400 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1109 14:43:15.976171   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1109 14:43:15.985230   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1109 14:43:16.440458   14400 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58240 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I1109 14:43:17.916965   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1109 14:43:17.916965   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1109 14:43:17.957413   14400 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1109 14:43:18.341564   14400 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1109 14:43:18.411545   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1109 14:43:18.411545   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1109 14:43:19.028419   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1109 14:43:19.028419   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1109 14:43:20.540118   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1109 14:43:20.540118   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1109 14:43:21.116416   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I1109 14:43:21.116416   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1109 14:43:22.596027   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1109 14:43:22.596027   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1109 14:43:23.109410   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1109 14:43:23.109410   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1109 14:43:24.344796   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1109 14:43:24.344796   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1109 14:43:25.338214   14400 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1109 14:43:25.338214   14400 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1109 14:43:25.828857   14400 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (11.4100578s)
I1109 14:43:25.829379   14400 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (10.2644331s)
I1109 14:43:25.830665   14400 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1109 14:43:25.842729   14400 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1109 14:43:26.022047   14400 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1109 14:43:26.079556   14400 api_server.go:52] waiting for apiserver process to appear ...
I1109 14:43:26.099075   14400 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1109 14:43:31.858439   14400 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (13.9010257s)
I1109 14:43:31.858439   14400 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (13.5168751s)
I1109 14:43:32.293521   14400 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (6.1944466s)
I1109 14:43:32.293521   14400 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (6.2714738s)
I1109 14:43:32.294032   14400 api_server.go:72] duration metric: took 16.7535324s to wait for apiserver process to appear ...
I1109 14:43:32.294032   14400 api_server.go:88] waiting for apiserver healthz status ...
I1109 14:43:32.294052   14400 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58239/healthz ...
I1109 14:43:32.295961   14400 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1109 14:43:32.298085   14400 out.go:177] * Enabled addons: storage-provisioner, default-storageclass, dashboard
I1109 14:43:32.299820   14400 addons.go:502] enable addons completed in 17.9139964s: enabled=[storage-provisioner default-storageclass dashboard]
I1109 14:43:32.313170   14400 api_server.go:279] https://127.0.0.1:58239/healthz returned 200:
ok
I1109 14:43:32.317177   14400 api_server.go:141] control plane version: v1.27.4
I1109 14:43:32.317177   14400 api_server.go:131] duration metric: took 23.1445ms to wait for apiserver health ...
I1109 14:43:32.317177   14400 system_pods.go:43] waiting for kube-system pods to appear ...
I1109 14:43:32.338817   14400 system_pods.go:59] 7 kube-system pods found
I1109 14:43:32.338817   14400 system_pods.go:61] "coredns-5d78c9869d-j65kd" [6fdba634-e05e-494d-bafc-cb3f69f0daa2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1109 14:43:32.338817   14400 system_pods.go:61] "etcd-minikube" [372d77a0-3e3e-4b30-ae2b-f1feed674024] Running
I1109 14:43:32.338817   14400 system_pods.go:61] "kube-apiserver-minikube" [8587dfbf-7f7e-4458-bab9-6c418a61af46] Running
I1109 14:43:32.338817   14400 system_pods.go:61] "kube-controller-manager-minikube" [614a6e6f-63e2-45f4-a484-59432267ccfc] Running
I1109 14:43:32.338817   14400 system_pods.go:61] "kube-proxy-km2wn" [898207e3-ac85-4c73-81eb-4f066dde7965] Running
I1109 14:43:32.338817   14400 system_pods.go:61] "kube-scheduler-minikube" [a7fd0196-78d6-4e84-9180-e2f47cb657d9] Running
I1109 14:43:32.338817   14400 system_pods.go:61] "storage-provisioner" [714474ba-8971-4f5e-aed4-f51cad11fb15] Running
I1109 14:43:32.338817   14400 system_pods.go:74] duration metric: took 21.6401ms to wait for pod list to return data ...
I1109 14:43:32.338817   14400 kubeadm.go:581] duration metric: took 16.798828s to wait for : map[apiserver:true system_pods:true] ...
I1109 14:43:32.338817   14400 node_conditions.go:102] verifying NodePressure condition ...
I1109 14:43:32.351597   14400 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1109 14:43:32.351597   14400 node_conditions.go:123] node cpu capacity is 4
I1109 14:43:32.352120   14400 node_conditions.go:105] duration metric: took 13.3033ms to run NodePressure ...
I1109 14:43:32.352179   14400 start.go:228] waiting for startup goroutines ...
I1109 14:43:32.352179   14400 start.go:233] waiting for cluster config update ...
I1109 14:43:32.352179   14400 start.go:242] writing updated cluster config ...
I1109 14:43:32.373864   14400 ssh_runner.go:195] Run: rm -f paused
I1109 14:43:32.578494   14400 start.go:600] kubectl: 1.27.2, cluster: 1.27.4 (minor skew: 0)
I1109 14:43:32.580673   14400 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Nov 09 10:12:02 minikube cri-dockerd[1211]: time="2023-11-09T10:12:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: 3da4cd537b47: Downloading [==============================================>    ]  17.04MB/18.48MB"
Nov 09 10:12:12 minikube cri-dockerd[1211]: time="2023-11-09T10:12:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: 3da4cd537b47: Downloading [================================================>  ]  17.81MB/18.48MB"
Nov 09 10:12:22 minikube cri-dockerd[1211]: time="2023-11-09T10:12:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: 05535d57e64c: Downloading [===============================================>   ]  40.47MB/43.04MB"
Nov 09 10:12:32 minikube cri-dockerd[1211]: time="2023-11-09T10:12:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: 05535d57e64c: Downloading [===============================================>   ]  40.91MB/43.04MB"
Nov 09 10:12:42 minikube cri-dockerd[1211]: time="2023-11-09T10:12:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: 05535d57e64c: Downloading [================================================>  ]  41.78MB/43.04MB"
Nov 09 10:12:52 minikube cri-dockerd[1211]: time="2023-11-09T10:12:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: 05535d57e64c: Downloading [=================================================> ]  42.21MB/43.04MB"
Nov 09 10:13:02 minikube cri-dockerd[1211]: time="2023-11-09T10:13:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: 05535d57e64c: Extracting [==================>                                ]  16.06MB/43.04MB"
Nov 09 10:13:12 minikube cri-dockerd[1211]: time="2023-11-09T10:13:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: 8bdddae2c8cc: Pull complete "
Nov 09 10:13:16 minikube cri-dockerd[1211]: time="2023-11-09T10:13:16Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.9.4@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:5b161f051d017e55d358435f295f5e9a297e66158f136321d9b04520ec6c48a3"
Nov 09 10:23:26 minikube cri-dockerd[1211]: time="2023-11-09T10:23:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bf9f9391f3832fb6c128a1cbd960061691fe5f47637b6ed929551f184c114f0d/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 09 10:23:26 minikube dockerd[957]: time="2023-11-09T10:23:26.939649143Z" level=warning msg="reference for unknown type: " digest="sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd" remote="registry.k8s.io/ingress-nginx/controller@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd"
Nov 09 10:23:28 minikube cri-dockerd[1211]: time="2023-11-09T10:23:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ef746b891c0688e3434163ce46dd1eb5226b60d846e5ba32528c8ad4f025a5f5/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 09 10:23:28 minikube cri-dockerd[1211]: time="2023-11-09T10:23:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a6f8a4b4a2ffd3f2e57f56288fa4ab6a45129ef8b017ed09fd4f1f2a99925c84/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 09 10:23:28 minikube dockerd[957]: time="2023-11-09T10:23:28.765659048Z" level=info msg="ignoring event" container=9c25072434557e0ad282030c0dbcf1a942531745a6174923e54d98fbfef9b5af module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 09 10:23:28 minikube dockerd[957]: time="2023-11-09T10:23:28.802433323Z" level=info msg="ignoring event" container=33fbf3ba2c023cdc4239b37bf5aa018e9a1b18a9906340d8f50e14b1a4d81ba7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 09 10:23:30 minikube dockerd[957]: time="2023-11-09T10:23:30.601600755Z" level=info msg="ignoring event" container=a6f8a4b4a2ffd3f2e57f56288fa4ab6a45129ef8b017ed09fd4f1f2a99925c84 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 09 10:23:31 minikube dockerd[957]: time="2023-11-09T10:23:31.638295620Z" level=info msg="ignoring event" container=ef746b891c0688e3434163ce46dd1eb5226b60d846e5ba32528c8ad4f025a5f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 09 10:23:36 minikube dockerd[957]: time="2023-11-09T10:23:36.700344914Z" level=info msg="ignoring event" container=0a6c1e34bc19284c3f33a4333dd1a76d14c073c2a24c38b22d19d6037e1b89da module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 09 10:23:36 minikube dockerd[957]: time="2023-11-09T10:23:36.982797692Z" level=info msg="ignoring event" container=76696029a74bae93b309a1a6845626b4ac9fd7a5161c19379dbaebb47995f55b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 09 10:23:38 minikube cri-dockerd[1211]: time="2023-11-09T10:23:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 8a49fdb3b6a5: Downloading [==========>                                        ]  730.5kB/3.397MB"
Nov 09 10:23:48 minikube cri-dockerd[1211]: time="2023-11-09T10:23:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 8a49fdb3b6a5: Downloading [======================>                            ]  1.496MB/3.397MB"
Nov 09 10:23:58 minikube cri-dockerd[1211]: time="2023-11-09T10:23:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [==>                                                ]  2.176MB/43.09MB"
Nov 09 10:24:08 minikube cri-dockerd[1211]: time="2023-11-09T10:24:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 8a49fdb3b6a5: Downloading [============================================>      ]  3.028MB/3.397MB"
Nov 09 10:24:18 minikube cri-dockerd[1211]: time="2023-11-09T10:24:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [========>                                          ]  3.846MB/21.52MB"
Nov 09 10:24:28 minikube cri-dockerd[1211]: time="2023-11-09T10:24:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [========>                                          ]  1.669MB/10.02MB"
Nov 09 10:24:38 minikube cri-dockerd[1211]: time="2023-11-09T10:24:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [============>                                      ]  2.504MB/10.02MB"
Nov 09 10:24:48 minikube cri-dockerd[1211]: time="2023-11-09T10:24:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [=================>                                 ]  3.549MB/10.02MB"
Nov 09 10:24:58 minikube cri-dockerd[1211]: time="2023-11-09T10:24:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [=======================>                           ]  4.698MB/10.02MB"
Nov 09 10:25:08 minikube cri-dockerd[1211]: time="2023-11-09T10:25:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [==============================>                    ]  6.055MB/10.02MB"
Nov 09 10:25:18 minikube cri-dockerd[1211]: time="2023-11-09T10:25:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [=================================>                 ]  6.786MB/10.02MB"
Nov 09 10:25:28 minikube cri-dockerd[1211]: time="2023-11-09T10:25:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [======================================>            ]  7.622MB/10.02MB"
Nov 09 10:25:38 minikube cri-dockerd[1211]: time="2023-11-09T10:25:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [==========================================>        ]  8.562MB/10.02MB"
Nov 09 10:25:48 minikube cri-dockerd[1211]: time="2023-11-09T10:25:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [===============================================>   ]  9.502MB/10.02MB"
Nov 09 10:25:58 minikube cri-dockerd[1211]: time="2023-11-09T10:25:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 1c50f5d1f8fa: Downloading [=====>                                             ]  313.5kB/2.842MB"
Nov 09 10:26:08 minikube cri-dockerd[1211]: time="2023-11-09T10:26:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 1c50f5d1f8fa: Downloading [==========================>                        ]  1.497MB/2.842MB"
Nov 09 10:26:18 minikube cri-dockerd[1211]: time="2023-11-09T10:26:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [=================================>                 ]  14.26MB/21.52MB"
Nov 09 10:26:28 minikube cri-dockerd[1211]: time="2023-11-09T10:26:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [==>                                                ]  555.5kB/12.89MB"
Nov 09 10:26:38 minikube cri-dockerd[1211]: time="2023-11-09T10:26:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [=======>                                           ]  1.948MB/12.89MB"
Nov 09 10:26:48 minikube cri-dockerd[1211]: time="2023-11-09T10:26:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [=============>                                     ]   3.48MB/12.89MB"
Nov 09 10:26:58 minikube cri-dockerd[1211]: time="2023-11-09T10:26:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [=================>                                 ]  4.594MB/12.89MB"
Nov 09 10:27:08 minikube cri-dockerd[1211]: time="2023-11-09T10:27:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [==============================================>    ]  19.91MB/21.52MB"
Nov 09 10:27:18 minikube cri-dockerd[1211]: time="2023-11-09T10:27:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [================================================>  ]  20.82MB/21.52MB"
Nov 09 10:27:28 minikube cri-dockerd[1211]: time="2023-11-09T10:27:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [===============================>                   ]  8.215MB/12.89MB"
Nov 09 10:27:38 minikube cri-dockerd[1211]: time="2023-11-09T10:27:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [====================================>              ]  9.468MB/12.89MB"
Nov 09 10:27:48 minikube cri-dockerd[1211]: time="2023-11-09T10:27:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [===========================>                       ]  23.94MB/43.09MB"
Nov 09 10:27:58 minikube cri-dockerd[1211]: time="2023-11-09T10:27:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [============================================>      ]  11.56MB/12.89MB"
Nov 09 10:28:08 minikube cri-dockerd[1211]: time="2023-11-09T10:28:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [================================================>  ]  12.53MB/12.89MB"
Nov 09 10:28:18 minikube cri-dockerd[1211]: time="2023-11-09T10:28:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [===========>                                       ]  4.403MB/18.48MB"
Nov 09 10:28:28 minikube cri-dockerd[1211]: time="2023-11-09T10:28:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [================================>                  ]  27.85MB/43.09MB"
Nov 09 10:28:38 minikube cri-dockerd[1211]: time="2023-11-09T10:28:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [=================================>                 ]  29.16MB/43.09MB"
Nov 09 10:28:48 minikube cri-dockerd[1211]: time="2023-11-09T10:28:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [====================>                              ]  7.466MB/18.48MB"
Nov 09 10:28:58 minikube cri-dockerd[1211]: time="2023-11-09T10:28:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [========================>                          ]  8.998MB/18.48MB"
Nov 09 10:29:08 minikube cri-dockerd[1211]: time="2023-11-09T10:29:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [===========================>                       ]  10.34MB/18.48MB"
Nov 09 10:29:18 minikube cri-dockerd[1211]: time="2023-11-09T10:29:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [===============================>                   ]  11.49MB/18.48MB"
Nov 09 10:29:28 minikube cri-dockerd[1211]: time="2023-11-09T10:29:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [==================================>                ]  12.83MB/18.48MB"
Nov 09 10:29:38 minikube cri-dockerd[1211]: time="2023-11-09T10:29:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [=====================================>             ]  13.79MB/18.48MB"
Nov 09 10:29:48 minikube cri-dockerd[1211]: time="2023-11-09T10:29:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [=======================================>           ]  14.74MB/18.48MB"
Nov 09 10:29:58 minikube cri-dockerd[1211]: time="2023-11-09T10:29:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [============================================>      ]  38.73MB/43.09MB"
Nov 09 10:30:08 minikube cri-dockerd[1211]: time="2023-11-09T10:30:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [==============================================>    ]  40.04MB/43.09MB"
Nov 09 10:30:18 minikube cri-dockerd[1211]: time="2023-11-09T10:30:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [=================================================> ]  18.19MB/18.48MB"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
33fbf3ba2c023       7e7451bb70423       6 minutes ago       Exited              patch                       0                   a6f8a4b4a2ffd       ingress-nginx-admission-patch-95jzt
9c25072434557       7e7451bb70423       6 minutes ago       Exited              create                      0                   ef746b891c068       ingress-nginx-admission-create-2q5n8
6fe6ed5a07f34       918bdd5e696a4       42 minutes ago      Running             backend-one                 0                   12a340f24e5be       backend-deployment-558b47c6c8-w4jh9
360a8d3305caf       918bdd5e696a4       42 minutes ago      Running             backend-one                 0                   b0d5f4ee08d62       backend-deployment-558b47c6c8-qtzk5
1d70774edad02       918bdd5e696a4       42 minutes ago      Running             backend-one                 0                   25c153da243bb       backend-deployment-558b47c6c8-qql5d
abbb1aacb5700       07655ddf2eebe       2 hours ago         Running             kubernetes-dashboard        16                  3e8309253bf2f       kubernetes-dashboard-5c5cfc8747-knr6h
f28846e9c2eb2       6e38f40d628db       2 hours ago         Running             storage-provisioner         23                  090850e3c1c28       storage-provisioner
e848ffa571ff8       115053965e86b       2 hours ago         Running             dashboard-metrics-scraper   9                   92efeaabf5bf8       dashboard-metrics-scraper-5dd9cbfd69-r2wx5
7a8dfee28d198       ead0a4a53df89       2 hours ago         Running             coredns                     11                  431c284dce293       coredns-5d78c9869d-j65kd
365b39d70a5fd       07655ddf2eebe       2 hours ago         Exited              kubernetes-dashboard        15                  3e8309253bf2f       kubernetes-dashboard-5c5cfc8747-knr6h
29ca89f09296f       6e38f40d628db       2 hours ago         Exited              storage-provisioner         22                  090850e3c1c28       storage-provisioner
dc8be74ea1b1d       6848d7eda0341       2 hours ago         Running             kube-proxy                  11                  2b8fb3ed4c12f       kube-proxy-km2wn
f257ecf28c9c5       f466468864b7a       2 hours ago         Running             kube-controller-manager     13                  30d1f000e8159       kube-controller-manager-minikube
2ae678d1467b0       86b6af7dd652c       2 hours ago         Running             etcd                        11                  aba7498185945       etcd-minikube
73434ec502804       e7972205b6614       2 hours ago         Running             kube-apiserver              11                  b0b8087d8a008       kube-apiserver-minikube
41c330632ed9b       98ef2570f3cde       2 hours ago         Running             kube-scheduler              11                  0e8e673c29616       kube-scheduler-minikube
8fe1dc783679a       ead0a4a53df89       3 hours ago         Exited              coredns                     10                  c35f5c07afe3f       coredns-5d78c9869d-j65kd
6caf582bc5626       115053965e86b       3 hours ago         Exited              dashboard-metrics-scraper   8                   8d7dacf111b7c       dashboard-metrics-scraper-5dd9cbfd69-r2wx5
499fad4f9858d       6848d7eda0341       3 hours ago         Exited              kube-proxy                  10                  5e8b09b6d1b6e       kube-proxy-km2wn
a85d3b7f88a39       86b6af7dd652c       3 hours ago         Exited              etcd                        10                  bd918011244ae       etcd-minikube
5ff8aacb14749       98ef2570f3cde       3 hours ago         Exited              kube-scheduler              10                  60b4b0c08739d       kube-scheduler-minikube
5b1a4d3f7038c       f466468864b7a       3 hours ago         Exited              kube-controller-manager     12                  b8126cbcb71bb       kube-controller-manager-minikube
a496aa10b36d5       e7972205b6614       3 hours ago         Exited              kube-apiserver              10                  d0d6d43d50870       kube-apiserver-minikube

* 
* ==> coredns [7a8dfee28d19] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:46873 - 10069 "HINFO IN 1598594516070528398.8799607456110793880. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.304648293s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": net/http: TLS handshake timeout

* 
* ==> coredns [8fe1dc783679] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:51181 - 60898 "HINFO IN 104690367219161007.355310589538626846. udp 55 false 512" NXDOMAIN qr,rd,ra 55 0.202385328s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": net/http: TLS handshake timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_05T15_38_39_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 05 Nov 2023 09:38:23 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 09 Nov 2023 10:30:15 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 09 Nov 2023 10:28:58 +0000   Sun, 05 Nov 2023 09:38:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 09 Nov 2023 10:28:58 +0000   Sun, 05 Nov 2023 09:38:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 09 Nov 2023 10:28:58 +0000   Sun, 05 Nov 2023 09:38:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 09 Nov 2023 10:28:58 +0000   Sun, 05 Nov 2023 09:38:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3965716Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3965716Ki
  pods:               110
System Info:
  Machine ID:                 918e946d76124a29b7f053ccec966026
  System UUID:                918e946d76124a29b7f053ccec966026
  Boot ID:                    5fcc0408-0e2d-41ae-b523-a68c2defeec7
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     backend-deployment-558b47c6c8-qql5d           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         42m
  default                     backend-deployment-558b47c6c8-qtzk5           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         42m
  default                     backend-deployment-558b47c6c8-w4jh9           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         42m
  ingress-nginx               ingress-nginx-controller-7799c6795f-lq226     100m (2%!)(MISSING)     0 (0%!)(MISSING)      90Mi (2%!)(MISSING)        0 (0%!)(MISSING)         6m59s
  kube-system                 coredns-5d78c9869d-j65kd                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     4d
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         4d
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d
  kube-system                 kube-proxy-km2wn                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d
  kubernetes-dashboard        dashboard-metrics-scraper-5dd9cbfd69-r2wx5    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d23h
  kubernetes-dashboard        kubernetes-dashboard-5c5cfc8747-knr6h         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (6%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* 
[  +0.001262] WSL (1) ERROR: ConfigMountFsTab:2483: Processing fstab with mount -a failed.
[  +0.024093] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000008]  failed 2
[  +0.045494] 9pnet_virtio: no channels available for device drvfs
[  +0.001106] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.116542] 9pnet_virtio: no channels available for device drvfs
[  +0.001328] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.126177] 9pnet_virtio: no channels available for device drvfs
[  +0.001055] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.084789] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.035258] 9pnet_virtio: no channels available for device drvfs
[  +0.001105] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.135399] 9pnet_virtio: no channels available for device drvfs
[  +0.001897] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.100183] 9pnet_virtio: no channels available for device drvfs
[  +0.207499] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Dhaka not found. Is the tzdata package installed?
[  +0.121386] Exception: 
[  +0.000008] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +1.941340] FS-Cache: Duplicate cookie detected
[  +0.001122] FS-Cache: O-cookie c=00000041 [p=00000002 fl=222 nc=0 na=1]
[  +0.000876] FS-Cache: O-cookie d=0000000034f6eae0{9P.session} n=00000000681b55a3
[  +0.000899] FS-Cache: O-key=[10] '34323934393430373234'
[  +0.000812] FS-Cache: N-cookie c=00000042 [p=00000002 fl=2 nc=0 na=1]
[  +0.000810] FS-Cache: N-cookie d=0000000034f6eae0{9P.session} n=00000000a30b9d8b
[  +0.000810] FS-Cache: N-key=[10] '34323934393430373234'
[  +0.005079] FS-Cache: Duplicate cookie detected
[  +0.000788] FS-Cache: O-cookie c=00000043 [p=00000002 fl=222 nc=0 na=1]
[  +0.000748] FS-Cache: O-cookie d=0000000034f6eae0{9P.session} n=000000004aa73c75
[  +0.000873] FS-Cache: O-key=[10] '34323934393430373235'
[  +0.002858] FS-Cache: N-cookie c=00000044 [p=00000002 fl=2 nc=0 na=1]
[  +0.002329] FS-Cache: N-cookie d=0000000034f6eae0{9P.session} n=0000000081e3cb87
[  +0.001773] FS-Cache: N-key=[10] '34323934393430373235'
[  +0.348106] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.519348] FS-Cache: Duplicate cookie detected
[  +0.001009] FS-Cache: O-cookie c=0000004b [p=00000002 fl=222 nc=0 na=1]
[  +0.000758] FS-Cache: O-cookie d=0000000034f6eae0{9P.session} n=00000000d4fefec0
[  +0.000905] FS-Cache: O-key=[10] '34323934393430383133'
[  +0.000600] FS-Cache: N-cookie c=0000004c [p=00000002 fl=2 nc=0 na=1]
[  +0.000725] FS-Cache: N-cookie d=0000000034f6eae0{9P.session} n=00000000b196c6c6
[  +0.000934] FS-Cache: N-key=[10] '34323934393430383133'
[  +0.008327] FS-Cache: Duplicate cookie detected
[  +0.000900] FS-Cache: O-cookie c=0000004d [p=00000002 fl=222 nc=0 na=1]
[  +0.000814] FS-Cache: O-cookie d=0000000034f6eae0{9P.session} n=0000000014b9689c
[  +0.000963] FS-Cache: O-key=[10] '34323934393430383134'
[  +0.000595] FS-Cache: N-cookie c=0000004e [p=00000002 fl=2 nc=0 na=1]
[  +0.000727] FS-Cache: N-cookie d=0000000034f6eae0{9P.session} n=0000000050a71491
[  +0.000918] FS-Cache: N-key=[10] '34323934393430383134'
[  +0.196956] new mount options do not match the existing superblock, will be ignored
[  +0.245290] Failed to connect to bus: No such file or directory
[Nov 9 07:42] Failed to connect to bus: No such file or directory
[  +0.254195] Failed to connect to bus: No such file or directory
[  +0.254655] Failed to connect to bus: No such file or directory
[  +0.256384] Failed to connect to bus: No such file or directory
[  +0.256610] Failed to connect to bus: No such file or directory
[  +0.551841] systemd-journald[55]: File /var/log/journal/c09e4efb05604387b9a3c14cd4f64a2e/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +8.252298] WSL (2) ERROR: WaitForBootProcess:3184: /sbin/init failed to start within 10000
[  +0.000010] ms
[Nov 9 09:03] hrtimer: interrupt took 1129894 ns

* 
* ==> etcd [2ae678d1467b] <==
* {"level":"info","ts":"2023-11-09T10:13:09.270Z","caller":"traceutil/trace.go:171","msg":"trace[1761327668] transaction","detail":"{read_only:false; response_revision:27845; number_of_response:1; }","duration":"105.911217ms","start":"2023-11-09T10:13:09.164Z","end":"2023-11-09T10:13:09.270Z","steps":["trace[1761327668] 'process raft request'  (duration: 105.711714ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:13:17.612Z","caller":"traceutil/trace.go:171","msg":"trace[1336016673] transaction","detail":"{read_only:false; response_revision:27853; number_of_response:1; }","duration":"203.667208ms","start":"2023-11-09T10:13:17.409Z","end":"2023-11-09T10:13:17.612Z","steps":["trace[1336016673] 'process raft request'  (duration: 203.501605ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:13:19.744Z","caller":"traceutil/trace.go:171","msg":"trace[1198622614] transaction","detail":"{read_only:false; response_revision:27854; number_of_response:1; }","duration":"121.726318ms","start":"2023-11-09T10:13:19.622Z","end":"2023-11-09T10:13:19.744Z","steps":["trace[1198622614] 'process raft request'  (duration: 121.548616ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:14:47.304Z","caller":"traceutil/trace.go:171","msg":"trace[1908542713] transaction","detail":"{read_only:false; response_revision:27954; number_of_response:1; }","duration":"153.682737ms","start":"2023-11-09T10:14:47.150Z","end":"2023-11-09T10:14:47.304Z","steps":["trace[1908542713] 'process raft request'  (duration: 153.203705ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:15:34.423Z","caller":"traceutil/trace.go:171","msg":"trace[1531408676] linearizableReadLoop","detail":"{readStateIndex:34662; appliedIndex:34660; }","duration":"135.536812ms","start":"2023-11-09T10:15:34.287Z","end":"2023-11-09T10:15:34.423Z","steps":["trace[1531408676] 'read index received'  (duration: 48.023673ms)","trace[1531408676] 'applied index is now lower than readState.Index'  (duration: 87.512239ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-09T10:15:34.423Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"135.876221ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:421"}
{"level":"info","ts":"2023-11-09T10:15:34.424Z","caller":"traceutil/trace.go:171","msg":"trace[2126452408] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:27997; }","duration":"137.137652ms","start":"2023-11-09T10:15:34.287Z","end":"2023-11-09T10:15:34.424Z","steps":["trace[2126452408] 'agreement among raft nodes before linearized reading'  (duration: 135.797919ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:15:34.423Z","caller":"traceutil/trace.go:171","msg":"trace[458442789] transaction","detail":"{read_only:false; response_revision:27996; number_of_response:1; }","duration":"171.413189ms","start":"2023-11-09T10:15:34.252Z","end":"2023-11-09T10:15:34.423Z","steps":["trace[458442789] 'process raft request'  (duration: 83.48364ms)","trace[458442789] 'compare'  (duration: 87.271433ms)"],"step_count":2}
{"level":"info","ts":"2023-11-09T10:15:34.423Z","caller":"traceutil/trace.go:171","msg":"trace[728162915] transaction","detail":"{read_only:false; response_revision:27997; number_of_response:1; }","duration":"158.159865ms","start":"2023-11-09T10:15:34.265Z","end":"2023-11-09T10:15:34.423Z","steps":["trace[728162915] 'process raft request'  (duration: 157.94626ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:16:07.034Z","caller":"traceutil/trace.go:171","msg":"trace[1124430350] transaction","detail":"{read_only:false; response_revision:28026; number_of_response:1; }","duration":"115.929581ms","start":"2023-11-09T10:16:06.918Z","end":"2023-11-09T10:16:07.034Z","steps":["trace[1124430350] 'process raft request'  (duration: 115.729502ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:16:07.979Z","caller":"traceutil/trace.go:171","msg":"trace[1676160206] linearizableReadLoop","detail":"{readStateIndex:34698; appliedIndex:34697; }","duration":"112.211454ms","start":"2023-11-09T10:16:07.867Z","end":"2023-11-09T10:16:07.979Z","steps":["trace[1676160206] 'read index received'  (duration: 112.11154ms)","trace[1676160206] 'applied index is now lower than readState.Index'  (duration: 98.514µs)"],"step_count":2}
{"level":"info","ts":"2023-11-09T10:16:07.979Z","caller":"traceutil/trace.go:171","msg":"trace[1486094107] transaction","detail":"{read_only:false; response_revision:28027; number_of_response:1; }","duration":"123.518553ms","start":"2023-11-09T10:16:07.856Z","end":"2023-11-09T10:16:07.979Z","steps":["trace[1486094107] 'process raft request'  (duration: 123.289721ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T10:16:07.979Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.459488ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-09T10:16:07.979Z","caller":"traceutil/trace.go:171","msg":"trace[1342984139] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:28027; }","duration":"112.519396ms","start":"2023-11-09T10:16:07.867Z","end":"2023-11-09T10:16:07.979Z","steps":["trace[1342984139] 'agreement among raft nodes before linearized reading'  (duration: 112.387678ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:16:25.393Z","caller":"traceutil/trace.go:171","msg":"trace[1311011348] transaction","detail":"{read_only:false; response_revision:28043; number_of_response:1; }","duration":"115.268706ms","start":"2023-11-09T10:16:25.278Z","end":"2023-11-09T10:16:25.393Z","steps":["trace[1311011348] 'process raft request'  (duration: 115.062809ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:17:36.836Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27820}
{"level":"info","ts":"2023-11-09T10:17:36.838Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":27820,"took":"1.109612ms","hash":3221725393}
{"level":"info","ts":"2023-11-09T10:17:36.838Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3221725393,"revision":27820,"compact-revision":27582}
{"level":"info","ts":"2023-11-09T10:17:52.892Z","caller":"traceutil/trace.go:171","msg":"trace[139993272] transaction","detail":"{read_only:false; response_revision:28124; number_of_response:1; }","duration":"124.373114ms","start":"2023-11-09T10:17:52.768Z","end":"2023-11-09T10:17:52.892Z","steps":["trace[139993272] 'process raft request'  (duration: 124.146105ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:19:36.485Z","caller":"traceutil/trace.go:171","msg":"trace[1949102260] transaction","detail":"{read_only:false; response_revision:28221; number_of_response:1; }","duration":"110.356295ms","start":"2023-11-09T10:19:36.374Z","end":"2023-11-09T10:19:36.485Z","steps":["trace[1949102260] 'process raft request'  (duration: 110.133388ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:20:23.639Z","caller":"traceutil/trace.go:171","msg":"trace[2127726841] transaction","detail":"{read_only:false; response_revision:28264; number_of_response:1; }","duration":"136.15863ms","start":"2023-11-09T10:20:23.503Z","end":"2023-11-09T10:20:23.639Z","steps":["trace[2127726841] 'process raft request'  (duration: 135.918326ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:22:23.707Z","caller":"traceutil/trace.go:171","msg":"trace[1630129065] transaction","detail":"{read_only:false; response_revision:28374; number_of_response:1; }","duration":"217.830908ms","start":"2023-11-09T10:22:23.489Z","end":"2023-11-09T10:22:23.707Z","steps":["trace[1630129065] 'process raft request'  (duration: 217.678705ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:22:36.827Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28109}
{"level":"info","ts":"2023-11-09T10:22:36.829Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":28109,"took":"929.693µs","hash":2751968477}
{"level":"info","ts":"2023-11-09T10:22:36.829Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2751968477,"revision":28109,"compact-revision":27820}
{"level":"info","ts":"2023-11-09T10:23:24.508Z","caller":"traceutil/trace.go:171","msg":"trace[296706347] transaction","detail":"{read_only:false; response_revision:28473; number_of_response:1; }","duration":"109.796419ms","start":"2023-11-09T10:23:24.398Z","end":"2023-11-09T10:23:24.508Z","steps":["trace[296706347] 'process raft request'  (duration: 100.399723ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:23:24.508Z","caller":"traceutil/trace.go:171","msg":"trace[1427280594] linearizableReadLoop","detail":"{readStateIndex:35238; appliedIndex:35237; }","duration":"108.501205ms","start":"2023-11-09T10:23:24.399Z","end":"2023-11-09T10:23:24.508Z","steps":["trace[1427280594] 'read index received'  (duration: 99.576814ms)","trace[1427280594] 'applied index is now lower than readState.Index'  (duration: 8.922991ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-09T10:23:24.509Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.618317ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-key-holder\" ","response":"range_response_count:1 size:3258"}
{"level":"info","ts":"2023-11-09T10:23:24.509Z","caller":"traceutil/trace.go:171","msg":"trace[1921037084] range","detail":"{range_begin:/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-key-holder; range_end:; response_count:1; response_revision:28473; }","duration":"109.719818ms","start":"2023-11-09T10:23:24.399Z","end":"2023-11-09T10:23:24.509Z","steps":["trace[1921037084] 'agreement among raft nodes before linearized reading'  (duration: 109.527316ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:23:24.527Z","caller":"traceutil/trace.go:171","msg":"trace[977783638] transaction","detail":"{read_only:false; response_revision:28475; number_of_response:1; }","duration":"110.062021ms","start":"2023-11-09T10:23:24.417Z","end":"2023-11-09T10:23:24.527Z","steps":["trace[977783638] 'process raft request'  (duration: 109.988021ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:23:24.528Z","caller":"traceutil/trace.go:171","msg":"trace[1630271698] transaction","detail":"{read_only:false; response_revision:28474; number_of_response:1; }","duration":"112.090842ms","start":"2023-11-09T10:23:24.415Z","end":"2023-11-09T10:23:24.527Z","steps":["trace[1630271698] 'process raft request'  (duration: 111.093932ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T10:23:24.541Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.01977ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/ingress-nginx/ingress-nginx-controller-7cb7b799bd-b46sn.1795eccdd483590c\" ","response":"range_response_count:1 size:752"}
{"level":"info","ts":"2023-11-09T10:23:24.541Z","caller":"traceutil/trace.go:171","msg":"trace[1362357703] range","detail":"{range_begin:/registry/events/ingress-nginx/ingress-nginx-controller-7cb7b799bd-b46sn.1795eccdd483590c; range_end:; response_count:1; response_revision:28476; }","duration":"105.126571ms","start":"2023-11-09T10:23:24.436Z","end":"2023-11-09T10:23:24.541Z","steps":["trace[1362357703] 'agreement among raft nodes before linearized reading'  (duration: 104.900468ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:23:24.739Z","caller":"traceutil/trace.go:171","msg":"trace[164126154] transaction","detail":"{read_only:false; response_revision:28478; number_of_response:1; }","duration":"137.822505ms","start":"2023-11-09T10:23:24.601Z","end":"2023-11-09T10:23:24.738Z","steps":["trace[164126154] 'process raft request'  (duration: 42.092529ms)","trace[164126154] 'compare'  (duration: 95.468673ms)"],"step_count":2}
{"level":"info","ts":"2023-11-09T10:23:24.740Z","caller":"traceutil/trace.go:171","msg":"trace[1419052239] linearizableReadLoop","detail":"{readStateIndex:35244; appliedIndex:35243; }","duration":"139.135017ms","start":"2023-11-09T10:23:24.601Z","end":"2023-11-09T10:23:24.740Z","steps":["trace[1419052239] 'read index received'  (duration: 41.870326ms)","trace[1419052239] 'applied index is now lower than readState.Index'  (duration: 97.261991ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-09T10:23:24.741Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"140.417231ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/ingress-nginx-controller-7799c6795f-lq226\" ","response":"range_response_count:1 size:5654"}
{"level":"info","ts":"2023-11-09T10:23:24.741Z","caller":"traceutil/trace.go:171","msg":"trace[2013131831] range","detail":"{range_begin:/registry/pods/ingress-nginx/ingress-nginx-controller-7799c6795f-lq226; range_end:; response_count:1; response_revision:28479; }","duration":"140.577432ms","start":"2023-11-09T10:23:24.601Z","end":"2023-11-09T10:23:24.741Z","steps":["trace[2013131831] 'agreement among raft nodes before linearized reading'  (duration: 140.240829ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T10:23:24.741Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.60719ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:136"}
{"level":"info","ts":"2023-11-09T10:23:24.742Z","caller":"traceutil/trace.go:171","msg":"trace[1983298744] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:28479; }","duration":"126.69499ms","start":"2023-11-09T10:23:24.615Z","end":"2023-11-09T10:23:24.742Z","steps":["trace[1983298744] 'agreement among raft nodes before linearized reading'  (duration: 126.521789ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:23:33.672Z","caller":"traceutil/trace.go:171","msg":"trace[147891382] linearizableReadLoop","detail":"{readStateIndex:35298; appliedIndex:35297; }","duration":"329.545259ms","start":"2023-11-09T10:23:33.343Z","end":"2023-11-09T10:23:33.672Z","steps":["trace[147891382] 'read index received'  (duration: 329.476958ms)","trace[147891382] 'applied index is now lower than readState.Index'  (duration: 67.101µs)"],"step_count":2}
{"level":"info","ts":"2023-11-09T10:23:33.673Z","caller":"traceutil/trace.go:171","msg":"trace[597426164] transaction","detail":"{read_only:false; response_revision:28529; number_of_response:1; }","duration":"426.400746ms","start":"2023-11-09T10:23:33.246Z","end":"2023-11-09T10:23:33.673Z","steps":["trace[597426164] 'process raft request'  (duration: 426.152544ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T10:23:33.673Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"329.823862ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:4 size:19435"}
{"level":"info","ts":"2023-11-09T10:23:33.673Z","caller":"traceutil/trace.go:171","msg":"trace[366745614] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:4; response_revision:28529; }","duration":"329.878662ms","start":"2023-11-09T10:23:33.343Z","end":"2023-11-09T10:23:33.673Z","steps":["trace[366745614] 'agreement among raft nodes before linearized reading'  (duration: 329.722561ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T10:23:33.673Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"232.705472ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-11-09T10:23:33.673Z","caller":"traceutil/trace.go:171","msg":"trace[1790162998] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:28529; }","duration":"232.952074ms","start":"2023-11-09T10:23:33.440Z","end":"2023-11-09T10:23:33.673Z","steps":["trace[1790162998] 'agreement among raft nodes before linearized reading'  (duration: 232.642871ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T10:23:33.682Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-09T10:23:33.343Z","time spent":"329.934263ms","remote":"127.0.0.1:59618","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":4,"response size":19459,"request content":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" "}
{"level":"warn","ts":"2023-11-09T10:23:33.841Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-09T10:23:33.246Z","time spent":"426.495147ms","remote":"127.0.0.1:59690","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4293,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/jobs/ingress-nginx/ingress-nginx-admission-create\" mod_revision:28527 > success:<request_put:<key:\"/registry/jobs/ingress-nginx/ingress-nginx-admission-create\" value_size:4226 >> failure:<request_range:<key:\"/registry/jobs/ingress-nginx/ingress-nginx-admission-create\" > >"}
{"level":"info","ts":"2023-11-09T10:23:40.288Z","caller":"traceutil/trace.go:171","msg":"trace[1649947832] transaction","detail":"{read_only:false; response_revision:28543; number_of_response:1; }","duration":"225.369604ms","start":"2023-11-09T10:23:40.062Z","end":"2023-11-09T10:23:40.288Z","steps":["trace[1649947832] 'process raft request'  (duration: 225.101497ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T10:23:40.746Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.359425ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13770"}
{"level":"info","ts":"2023-11-09T10:23:40.746Z","caller":"traceutil/trace.go:171","msg":"trace[1215335866] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:28543; }","duration":"136.673215ms","start":"2023-11-09T10:23:40.610Z","end":"2023-11-09T10:23:40.746Z","steps":["trace[1215335866] 'range keys from in-memory index tree'  (duration: 136.088919ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:23:47.614Z","caller":"traceutil/trace.go:171","msg":"trace[1359962028] transaction","detail":"{read_only:false; response_revision:28549; number_of_response:1; }","duration":"163.5588ms","start":"2023-11-09T10:23:47.450Z","end":"2023-11-09T10:23:47.614Z","steps":["trace[1359962028] 'process raft request'  (duration: 163.370199ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:25:03.633Z","caller":"traceutil/trace.go:171","msg":"trace[396617533] transaction","detail":"{read_only:false; response_revision:28609; number_of_response:1; }","duration":"139.949716ms","start":"2023-11-09T10:25:03.493Z","end":"2023-11-09T10:25:03.633Z","steps":["trace[396617533] 'process raft request'  (duration: 139.77571ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T10:27:36.839Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28388}
{"level":"info","ts":"2023-11-09T10:27:36.840Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":28388,"took":"1.181658ms","hash":1347449363}
{"level":"info","ts":"2023-11-09T10:27:36.840Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1347449363,"revision":28388,"compact-revision":28109}
{"level":"info","ts":"2023-11-09T10:29:35.111Z","caller":"traceutil/trace.go:171","msg":"trace[1800655821] linearizableReadLoop","detail":"{readStateIndex:35673; appliedIndex:35672; }","duration":"101.852058ms","start":"2023-11-09T10:29:35.009Z","end":"2023-11-09T10:29:35.111Z","steps":["trace[1800655821] 'read index received'  (duration: 101.608154ms)","trace[1800655821] 'applied index is now lower than readState.Index'  (duration: 242.804µs)"],"step_count":2}
{"level":"info","ts":"2023-11-09T10:29:35.111Z","caller":"traceutil/trace.go:171","msg":"trace[1987756483] transaction","detail":"{read_only:false; response_revision:28827; number_of_response:1; }","duration":"198.990545ms","start":"2023-11-09T10:29:34.912Z","end":"2023-11-09T10:29:35.111Z","steps":["trace[1987756483] 'process raft request'  (duration: 198.592539ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T10:29:35.317Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"308.661822ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-09T10:29:35.318Z","caller":"traceutil/trace.go:171","msg":"trace[2109477803] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:28827; }","duration":"308.771523ms","start":"2023-11-09T10:29:35.009Z","end":"2023-11-09T10:29:35.318Z","steps":["trace[2109477803] 'agreement among raft nodes before linearized reading'  (duration: 102.02216ms)","trace[2109477803] 'range keys from in-memory index tree'  (duration: 206.572361ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-09T10:29:35.318Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-09T10:29:35.009Z","time spent":"308.857025ms","remote":"127.0.0.1:59646","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}

* 
* ==> etcd [a85d3b7f88a3] <==
* {"level":"warn","ts":"2023-11-09T08:03:59.574Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-09T08:03:58.268Z","time spent":"1.306507537s","remote":"127.0.0.1:55810","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-11-09T08:05:17.152Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21769}
{"level":"info","ts":"2023-11-09T08:05:17.156Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21769,"took":"2.570034ms","hash":270853480}
{"level":"info","ts":"2023-11-09T08:05:17.156Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":270853480,"revision":21769,"compact-revision":21460}
{"level":"info","ts":"2023-11-09T08:08:31.537Z","caller":"traceutil/trace.go:171","msg":"trace[1590536667] linearizableReadLoop","detail":"{readStateIndex:27416; appliedIndex:27415; }","duration":"215.818127ms","start":"2023-11-09T08:08:31.321Z","end":"2023-11-09T08:08:31.537Z","steps":["trace[1590536667] 'read index received'  (duration: 215.667827ms)","trace[1590536667] 'applied index is now lower than readState.Index'  (duration: 148.6µs)"],"step_count":2}
{"level":"warn","ts":"2023-11-09T08:08:31.538Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"216.177628ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13434"}
{"level":"info","ts":"2023-11-09T08:08:31.538Z","caller":"traceutil/trace.go:171","msg":"trace[696285273] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:22168; }","duration":"216.272129ms","start":"2023-11-09T08:08:31.321Z","end":"2023-11-09T08:08:31.538Z","steps":["trace[696285273] 'agreement among raft nodes before linearized reading'  (duration: 216.043328ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:08:31.537Z","caller":"traceutil/trace.go:171","msg":"trace[1657023683] transaction","detail":"{read_only:false; response_revision:22168; number_of_response:1; }","duration":"331.020216ms","start":"2023-11-09T08:08:31.206Z","end":"2023-11-09T08:08:31.537Z","steps":["trace[1657023683] 'process raft request'  (duration: 330.505714ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T08:08:31.540Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-09T08:08:31.206Z","time spent":"333.064423ms","remote":"127.0.0.1:55530","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:22167 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-11-09T08:08:47.826Z","caller":"traceutil/trace.go:171","msg":"trace[1918591668] transaction","detail":"{read_only:false; response_revision:22182; number_of_response:1; }","duration":"114.03584ms","start":"2023-11-09T08:08:47.712Z","end":"2023-11-09T08:08:47.826Z","steps":["trace[1918591668] 'process raft request'  (duration: 113.778437ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:09:02.256Z","caller":"traceutil/trace.go:171","msg":"trace[1326141348] linearizableReadLoop","detail":"{readStateIndex:27446; appliedIndex:27445; }","duration":"125.486712ms","start":"2023-11-09T08:09:02.131Z","end":"2023-11-09T08:09:02.256Z","steps":["trace[1326141348] 'read index received'  (duration: 124.779514ms)","trace[1326141348] 'applied index is now lower than readState.Index'  (duration: 706.098µs)"],"step_count":2}
{"level":"info","ts":"2023-11-09T08:09:02.257Z","caller":"traceutil/trace.go:171","msg":"trace[1227645939] transaction","detail":"{read_only:false; response_revision:22192; number_of_response:1; }","duration":"180.982041ms","start":"2023-11-09T08:09:02.076Z","end":"2023-11-09T08:09:02.257Z","steps":["trace[1227645939] 'process raft request'  (duration: 179.735245ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T08:09:02.257Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.33171ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-11-09T08:09:02.257Z","caller":"traceutil/trace.go:171","msg":"trace[1564432585] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:22192; }","duration":"126.449009ms","start":"2023-11-09T08:09:02.131Z","end":"2023-11-09T08:09:02.257Z","steps":["trace[1564432585] 'agreement among raft nodes before linearized reading'  (duration: 126.21771ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:10:17.199Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22010}
{"level":"info","ts":"2023-11-09T08:10:17.203Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22010,"took":"2.94669ms","hash":920496733}
{"level":"info","ts":"2023-11-09T08:10:17.203Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":920496733,"revision":22010,"compact-revision":21769}
{"level":"info","ts":"2023-11-09T08:11:02.857Z","caller":"traceutil/trace.go:171","msg":"trace[1878748253] transaction","detail":"{read_only:false; response_revision:22291; number_of_response:1; }","duration":"128.840207ms","start":"2023-11-09T08:11:02.728Z","end":"2023-11-09T08:11:02.857Z","steps":["trace[1878748253] 'process raft request'  (duration: 128.635703ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:11:41.627Z","caller":"traceutil/trace.go:171","msg":"trace[609174322] transaction","detail":"{read_only:false; response_revision:22323; number_of_response:1; }","duration":"118.314478ms","start":"2023-11-09T08:11:41.509Z","end":"2023-11-09T08:11:41.627Z","steps":["trace[609174322] 'process raft request'  (duration: 118.148182ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:12:26.700Z","caller":"traceutil/trace.go:171","msg":"trace[2122175061] transaction","detail":"{read_only:false; response_revision:22359; number_of_response:1; }","duration":"114.700756ms","start":"2023-11-09T08:12:26.586Z","end":"2023-11-09T08:12:26.700Z","steps":["trace[2122175061] 'process raft request'  (duration: 114.471654ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:15:17.213Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22254}
{"level":"info","ts":"2023-11-09T08:15:17.217Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22254,"took":"3.03553ms","hash":2844592649}
{"level":"info","ts":"2023-11-09T08:15:17.217Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2844592649,"revision":22254,"compact-revision":22010}
{"level":"info","ts":"2023-11-09T08:15:47.477Z","caller":"traceutil/trace.go:171","msg":"trace[868904874] transaction","detail":"{read_only:false; response_revision:22521; number_of_response:1; }","duration":"136.07505ms","start":"2023-11-09T08:15:47.340Z","end":"2023-11-09T08:15:47.477Z","steps":["trace[868904874] 'process raft request'  (duration: 135.829551ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:20:17.238Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22497}
{"level":"info","ts":"2023-11-09T08:20:17.241Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22497,"took":"2.28893ms","hash":2224299957}
{"level":"info","ts":"2023-11-09T08:20:17.241Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2224299957,"revision":22497,"compact-revision":22254}
{"level":"info","ts":"2023-11-09T08:25:17.248Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22763}
{"level":"info","ts":"2023-11-09T08:25:17.251Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22763,"took":"1.840022ms","hash":3455147363}
{"level":"info","ts":"2023-11-09T08:25:17.252Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3455147363,"revision":22763,"compact-revision":22497}
{"level":"warn","ts":"2023-11-09T08:26:08.283Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128025028363573058,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-11-09T08:26:08.784Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128025028363573058,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-11-09T08:26:09.285Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128025028363573058,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-11-09T08:26:09.303Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.943741023s","expected-duration":"1s"}
{"level":"info","ts":"2023-11-09T08:26:09.304Z","caller":"traceutil/trace.go:171","msg":"trace[954677404] transaction","detail":"{read_only:false; response_revision:23050; number_of_response:1; }","duration":"1.944511628s","start":"2023-11-09T08:26:07.359Z","end":"2023-11-09T08:26:09.304Z","steps":["trace[954677404] 'process raft request'  (duration: 1.944361627s)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T08:26:09.305Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-09T08:26:07.359Z","time spent":"1.944809829s","remote":"127.0.0.1:55638","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:23042 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2023-11-09T08:26:09.405Z","caller":"traceutil/trace.go:171","msg":"trace[1152185612] linearizableReadLoop","detail":"{readStateIndex:28523; appliedIndex:28522; }","duration":"1.623621359s","start":"2023-11-09T08:26:07.782Z","end":"2023-11-09T08:26:09.405Z","steps":["trace[1152185612] 'read index received'  (duration: 1.523386644s)","trace[1152185612] 'applied index is now lower than readState.Index'  (duration: 100.232315ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-09T08:26:09.406Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.623993161s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"warn","ts":"2023-11-09T08:26:09.406Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.143794015s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-09T08:26:09.406Z","caller":"traceutil/trace.go:171","msg":"trace[826488864] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:23050; }","duration":"1.624060562s","start":"2023-11-09T08:26:07.782Z","end":"2023-11-09T08:26:09.406Z","steps":["trace[826488864] 'agreement among raft nodes before linearized reading'  (duration: 1.623890261s)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:26:09.406Z","caller":"traceutil/trace.go:171","msg":"trace[2001160194] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:23050; }","duration":"1.143880516s","start":"2023-11-09T08:26:08.262Z","end":"2023-11-09T08:26:09.406Z","steps":["trace[2001160194] 'agreement among raft nodes before linearized reading'  (duration: 1.143753015s)"],"step_count":1}
{"level":"warn","ts":"2023-11-09T08:26:09.406Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-09T08:26:08.262Z","time spent":"1.144133018s","remote":"127.0.0.1:55810","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-11-09T08:26:09.406Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-09T08:26:07.782Z","time spent":"1.624351663s","remote":"127.0.0.1:55530","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1136,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2023-11-09T08:26:52.598Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"273.730159ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-09T08:26:52.598Z","caller":"traceutil/trace.go:171","msg":"trace[135867487] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:23086; }","duration":"273.934961ms","start":"2023-11-09T08:26:52.324Z","end":"2023-11-09T08:26:52.598Z","steps":["trace[135867487] 'range keys from in-memory index tree'  (duration: 273.354557ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:26:52.730Z","caller":"traceutil/trace.go:171","msg":"trace[2072383216] transaction","detail":"{read_only:false; response_revision:23087; number_of_response:1; }","duration":"102.142269ms","start":"2023-11-09T08:26:52.628Z","end":"2023-11-09T08:26:52.730Z","steps":["trace[2072383216] 'process raft request'  (duration: 69.520523ms)","trace[2072383216] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:115; } (duration: 31.248935ms)"],"step_count":2}
{"level":"info","ts":"2023-11-09T08:30:17.259Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23011}
{"level":"info","ts":"2023-11-09T08:30:17.262Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23011,"took":"2.274912ms","hash":3323049279}
{"level":"info","ts":"2023-11-09T08:30:17.262Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3323049279,"revision":23011,"compact-revision":22763}
{"level":"info","ts":"2023-11-09T08:34:38.653Z","caller":"traceutil/trace.go:171","msg":"trace[1515556025] transaction","detail":"{read_only:false; response_revision:23535; number_of_response:1; }","duration":"128.246323ms","start":"2023-11-09T08:34:38.525Z","end":"2023-11-09T08:34:38.653Z","steps":["trace[1515556025] 'process raft request'  (duration: 127.98272ms)"],"step_count":1}
{"level":"info","ts":"2023-11-09T08:35:17.275Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23266}
{"level":"info","ts":"2023-11-09T08:35:17.277Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23266,"took":"1.142256ms","hash":2909618470}
{"level":"info","ts":"2023-11-09T08:35:17.277Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2909618470,"revision":23266,"compact-revision":23011}
{"level":"info","ts":"2023-11-09T08:36:27.221Z","caller":"traceutil/trace.go:171","msg":"trace[415354949] transaction","detail":"{read_only:false; response_revision:23666; number_of_response:1; }","duration":"112.224173ms","start":"2023-11-09T08:36:27.108Z","end":"2023-11-09T08:36:27.221Z","steps":["trace[415354949] 'process raft request'  (duration: 90.283888ms)","trace[415354949] 'compare'  (duration: 21.839983ms)"],"step_count":2}
{"level":"info","ts":"2023-11-09T08:38:40.805Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-11-09T08:38:40.811Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-11-09T08:38:41.095Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-11-09T08:38:41.125Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-09T08:38:41.126Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-09T08:38:41.127Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  10:30:23 up  2:48,  0 users,  load average: 1.23, 1.14, 1.05
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [73434ec50280] <==
* Trace[1768036639]: ---"Object stored in database" 565ms (08:43:08.478)
Trace[1768036639]: [584.175786ms] [584.175786ms] END
I1109 08:43:12.693821       1 trace.go:219] Trace[254119268]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e31413ee-e287-4b53-9790-321325db4e28,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/ingress-nginx/events/ingress-nginx-controller-7799c6795f-w268q.1795e7e091614d6b,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PATCH (09-Nov-2023 08:43:12.095) (total time: 598ms):
Trace[254119268]: ["GuaranteedUpdate etcd3" audit-id:e31413ee-e287-4b53-9790-321325db4e28,key:/events/ingress-nginx/ingress-nginx-controller-7799c6795f-w268q.1795e7e091614d6b,type:*core.Event,resource:events 597ms (08:43:12.096)
Trace[254119268]:  ---"initial value restored" 216ms (08:43:12.312)
Trace[254119268]:  ---"Txn call completed" 379ms (08:43:12.693)]
Trace[254119268]: ---"Object stored in database" 379ms (08:43:12.693)
Trace[254119268]: [598.502282ms] [598.502282ms] END
I1109 08:43:13.093267       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I1109 08:43:13.297931       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1109 08:43:13.629593       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I1109 08:43:13.818080       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1109 08:43:13.897424       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1109 08:43:19.793491       1 trace.go:219] Trace[1526425665]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a630cfb9-cd2a-4d2d-91d1-efe647c5676c,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/kubernetes,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (09-Nov-2023 08:43:19.192) (total time: 601ms):
Trace[1526425665]: ---"About to write a response" 600ms (08:43:19.793)
Trace[1526425665]: [601.166042ms] [601.166042ms] END
I1109 08:43:19.892093       1 trace.go:219] Trace[1215679735]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f2caa03d-9074-4e62-9c0f-c53c6b4e8187,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (09-Nov-2023 08:43:19.332) (total time: 559ms):
Trace[1215679735]: ["Create etcd3" audit-id:f2caa03d-9074-4e62-9c0f-c53c6b4e8187,key:/events/default/backend-deployment-558b47c6c8-vvkdz.1795e7e3a43f1231,type:*core.Event,resource:events 558ms (08:43:19.333)
Trace[1215679735]:  ---"Txn call succeeded" 558ms (08:43:19.891)]
Trace[1215679735]: [559.836039ms] [559.836039ms] END
I1109 08:43:20.396713       1 trace.go:219] Trace[1561610909]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Nov-2023 08:43:19.794) (total time: 602ms):
Trace[1561610909]: ---"initial value restored" 96ms (08:43:19.891)
Trace[1561610909]: ---"Transaction prepared" 199ms (08:43:20.090)
Trace[1561610909]: ---"Txn call completed" 305ms (08:43:20.396)
Trace[1561610909]: [602.105949ms] [602.105949ms] END
I1109 08:43:22.322091       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1109 08:43:22.324912       1 controller.go:624] quota admission added evaluator for: endpoints
I1109 09:44:13.896116       1 trace.go:219] Trace[1355302160]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:91fea563-a3f2-4275-a6bf-fcacb9f976e0,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (09-Nov-2023 09:29:36.192) (total time: 2870ms):
Trace[1355302160]: ["GuaranteedUpdate etcd3" audit-id:91fea563-a3f2-4275-a6bf-fcacb9f976e0,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 2869ms (09:29:36.192)
Trace[1355302160]:  ---"Txn call completed" 2868ms (09:44:13.895)]
Trace[1355302160]: [2.870262429s] [2.870262429s] END
I1109 09:44:13.897081       1 trace.go:219] Trace[787199769]: "Get" accept:application/json, */*,audit-id:55aeaea0-936b-4be8-8df3-c0ba72d1c9e9,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (09-Nov-2023 09:29:36.097) (total time: 2966ms):
Trace[787199769]: ---"About to write a response" 2966ms (09:44:13.896)
Trace[787199769]: [2.966309947s] [2.966309947s] END
I1109 09:44:13.898007       1 trace.go:219] Trace[1593166953]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:55b40eab-5f28-4b32-b135-88c3e5874603,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (09-Nov-2023 09:29:36.109) (total time: 2955ms):
Trace[1593166953]: ["GuaranteedUpdate etcd3" audit-id:55b40eab-5f28-4b32-b135-88c3e5874603,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 2954ms (09:29:36.109)
Trace[1593166953]:  ---"Txn call completed" 2953ms (09:44:13.897)]
Trace[1593166953]: [2.955201833s] [2.955201833s] END
I1109 09:44:13.958583       1 trace.go:219] Trace[1867831693]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9f937541-300a-4f00-9d4b-baa0ab28b80c,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (09-Nov-2023 09:29:38.571) (total time: 553ms):
Trace[1867831693]: ["Create etcd3" audit-id:9f937541-300a-4f00-9d4b-baa0ab28b80c,key:/events/kube-system/kube-apiserver-minikube.1795ea6abd842a12,type:*core.Event,resource:events 552ms (09:29:38.573)
Trace[1867831693]:  ---"TransformToStorage succeeded" 491ms (09:44:13.897)]
Trace[1867831693]: [553.777181ms] [553.777181ms] END
E1109 09:44:28.126949       1 authentication.go:70] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1109 09:44:28.461764       1 authentication.go:70] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
W1109 09:45:36.031927       1 dispatcher.go:208] Failed calling webhook, failing closed validate.nginx.ingress.kubernetes.io: failed calling webhook "validate.nginx.ingress.kubernetes.io": failed to call webhook: Post "https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s": dial tcp 10.110.58.132:443: connect: connection refused
I1109 09:48:19.332349       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1109 09:48:23.547299       1 alloc.go:330] "allocated clusterIPs" service="default/backend-service" clusterIPs=map[IPv4:10.97.24.45]
W1109 09:48:29.162384       1 dispatcher.go:208] Failed calling webhook, failing closed validate.nginx.ingress.kubernetes.io: failed calling webhook "validate.nginx.ingress.kubernetes.io": failed to call webhook: Post "https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s": service "ingress-nginx-controller-admission" not found
E1109 09:48:32.934747       1 authentication.go:70] "Unable to authenticate the request" err="[invalid bearer token, serviceaccounts \"ingress-nginx-admission\" not found]"
E1109 09:48:33.130268       1 authentication.go:70] "Unable to authenticate the request" err="[invalid bearer token, serviceaccounts \"ingress-nginx-admission\" not found]"
I1109 09:51:22.962558       1 controller.go:624] quota admission added evaluator for: namespaces
I1109 09:51:23.428856       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.98.105.185]
I1109 09:51:23.489397       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.110.246.12]
I1109 09:51:23.573365       1 controller.go:624] quota admission added evaluator for: jobs.batch
I1109 09:51:23.928741       1 controller.go:624] quota admission added evaluator for: networkpolicies.networking.k8s.io
I1109 10:23:33.855209       1 trace.go:219] Trace[1799131756]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:04f29afe-045a-4672-8583-989f1ba27edf,client:192.168.49.2,protocol:HTTP/2.0,resource:jobs,scope:resource,url:/apis/batch/v1/namespaces/ingress-nginx/jobs/ingress-nginx-admission-create/status,user-agent:kube-controller-manager/v1.27.4 (linux/amd64) kubernetes/fa3d799/system:serviceaccount:kube-system:job-controller,verb:PUT (09-Nov-2023 10:23:33.228) (total time: 616ms):
Trace[1799131756]: ["GuaranteedUpdate etcd3" audit-id:04f29afe-045a-4672-8583-989f1ba27edf,key:/jobs/ingress-nginx/ingress-nginx-admission-create,type:*batch.Job,resource:jobs.batch 615ms (10:23:33.229)
Trace[1799131756]:  ---"Txn call completed" 598ms (10:23:33.844)]
Trace[1799131756]: [616.351982ms] [616.351982ms] END
W1109 10:28:39.500280       1 dispatcher.go:208] Failed calling webhook, failing closed validate.nginx.ingress.kubernetes.io: failed calling webhook "validate.nginx.ingress.kubernetes.io": failed to call webhook: Post "https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s": dial tcp 10.110.246.12:443: connect: connection refused

* 
* ==> kube-apiserver [a496aa10b36d] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1109 08:38:50.887678       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1109 08:38:50.891221       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1109 08:38:50.942455       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1109 08:38:50.990884       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1109 08:38:50.993534       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1109 08:38:51.026165       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1109 08:38:51.029620       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [5b1a4d3f7038] <==
* I1109 07:30:36.640350       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1109 07:30:36.634272       1 shared_informer.go:318] Caches are synced for resource quota
I1109 07:30:36.634347       1 shared_informer.go:318] Caches are synced for job
I1109 07:30:36.634615       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1109 07:30:36.634632       1 shared_informer.go:318] Caches are synced for ephemeral
I1109 07:30:36.634676       1 shared_informer.go:318] Caches are synced for garbage collector
I1109 07:30:36.645131       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
W1109 07:30:36.806531       1 endpointslice_controller.go:297] Error syncing endpoint slices for service "default/backend-service", retrying. Error: EndpointSlice informer cache is out of date
W1109 07:30:36.808120       1 endpointslice_controller.go:297] Error syncing endpoint slices for service "default/backend-service", retrying. Error: EndpointSlice informer cache is out of date
W1109 07:30:36.807368       1 endpointslice_controller.go:297] Error syncing endpoint slices for service "kubernetes-dashboard/kubernetes-dashboard", retrying. Error: EndpointSlice informer cache is out of date
I1109 07:55:46.058915       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-7799c6795f to 1"
I1109 07:55:46.072252       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 07:55:46.088241       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7799c6795f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-7799c6795f-lfqjl"
I1109 07:55:46.130846       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 07:55:46.131671       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 07:55:46.133481       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-m4hrm"
I1109 07:55:46.165031       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 07:55:46.168647       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-wm9rq"
I1109 07:55:46.211675       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 07:55:46.214806       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 07:55:46.256727       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 07:55:46.257858       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 07:55:46.333840       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 07:55:46.414351       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:24:31.266118       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:24:45.135884       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:29:51.071964       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7799c6795f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-7799c6795f-hjck4"
I1109 08:32:32.026567       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:32:45.122089       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:33:03.230998       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:04.232305       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:04.303542       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:04.326637       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:04.337886       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
E1109 08:33:04.338395       1 job_controller.go:556] syncing job: failed pod(s) detected for job key "ingress-nginx/ingress-nginx-admission-create"
I1109 08:33:04.339466       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:05.327593       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:14.294304       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:14.294456       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-lbxl4"
I1109 08:33:14.308513       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:14.308686       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:33:14.330764       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 08:34:01.141285       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:02.141782       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:02.158354       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:02.169519       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:02.181371       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
E1109 08:34:02.181541       1 job_controller.go:556] syncing job: failed pod(s) detected for job key "ingress-nginx/ingress-nginx-admission-patch"
I1109 08:34:02.181707       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:03.170414       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:12.161488       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:12.163989       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-6f9m7"
I1109 08:34:12.176217       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:12.180084       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:12.323249       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 08:34:34.442490       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7799c6795f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-7799c6795f-w268q"
I1109 08:36:23.371521       1 event.go:307] "Event occurred" object="default/backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set backend-deployment-558b47c6c8 to 3"
I1109 08:36:23.412032       1 event.go:307] "Event occurred" object="default/backend-deployment-558b47c6c8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-558b47c6c8-vvkdz"
I1109 08:36:23.443221       1 event.go:307] "Event occurred" object="default/backend-deployment-558b47c6c8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-558b47c6c8-qmd87"
I1109 08:36:23.453610       1 event.go:307] "Event occurred" object="default/backend-deployment-558b47c6c8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-558b47c6c8-nv59d"

* 
* ==> kube-controller-manager [f257ecf28c9c] <==
* I1109 09:51:23.567367       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-7cb7b799bd to 1"
I1109 09:51:23.607036       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 09:51:23.622322       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7cb7b799bd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-7cb7b799bd-b46sn"
I1109 09:51:23.650927       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 09:51:23.654680       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-cvjzr"
I1109 09:51:23.720280       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 09:51:23.720951       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 09:51:23.720986       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 09:51:23.745116       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 09:51:23.748954       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-qllcq"
I1109 09:51:23.768825       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 09:51:23.774137       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 09:51:23.843842       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 09:51:23.936957       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:00:28.481141       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:00:29.646830       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:00:30.243689       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:00:30.658438       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:00:31.275701       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:00:31.313018       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:00:31.331871       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:00:31.338031       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1109 10:00:31.361567       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:00:31.750781       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:00:32.398161       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:00:32.433981       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:00:32.463733       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:00:32.464051       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1109 10:23:24.070877       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-7799c6795f to 1"
I1109 10:23:24.200574       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-7cb7b799bd to 0 from 1"
I1109 10:23:24.203227       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7799c6795f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-7799c6795f-lq226"
I1109 10:23:24.258684       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7cb7b799bd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-7cb7b799bd-b46sn"
I1109 10:23:26.881676       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:26.895852       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:26.908188       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:26.908961       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-2q5n8"
I1109 10:23:26.922611       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:26.929201       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:26.957434       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:26.958179       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:27.006704       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:27.025157       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:27.026958       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-95jzt"
I1109 10:23:27.039934       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:27.049646       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:27.139201       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:29.316864       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:29.340775       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:31.738067       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:32.113575       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:32.632579       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:32.656356       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:32.760049       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:32.798562       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:32.814744       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1109 10:23:32.818508       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1109 10:23:33.187847       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:33.224209       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:33.675191       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1109 10:23:33.859670       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"

* 
* ==> kube-proxy [499fad4f9858] <==
* I1109 07:30:40.095472       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1109 07:30:40.142493       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1109 07:30:40.142602       1 server_others.go:554] "Using iptables proxy"
I1109 07:30:41.846461       1 server_others.go:192] "Using iptables Proxier"
I1109 07:30:41.846715       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1109 07:30:41.846804       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1109 07:30:41.846893       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1109 07:30:41.847102       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1109 07:30:41.865748       1 server.go:658] "Version info" version="v1.27.4"
I1109 07:30:41.865815       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1109 07:30:41.979752       1 config.go:188] "Starting service config controller"
I1109 07:30:41.979789       1 shared_informer.go:311] Waiting for caches to sync for service config
I1109 07:30:41.979832       1 config.go:97] "Starting endpoint slice config controller"
I1109 07:30:41.979841       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1109 07:30:41.980560       1 config.go:315] "Starting node config controller"
I1109 07:30:41.980585       1 shared_informer.go:311] Waiting for caches to sync for node config
I1109 07:30:42.081501       1 shared_informer.go:318] Caches are synced for node config
I1109 07:30:42.081547       1 shared_informer.go:318] Caches are synced for service config
I1109 07:30:42.081558       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [dc8be74ea1b1] <==
* I1109 08:43:17.893157       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1109 08:43:17.894867       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1109 08:43:17.899648       1 server_others.go:554] "Using iptables proxy"
I1109 08:43:18.911268       1 server_others.go:192] "Using iptables Proxier"
I1109 08:43:18.911332       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1109 08:43:18.911353       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1109 08:43:18.911377       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1109 08:43:18.989732       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1109 08:43:18.997927       1 server.go:658] "Version info" version="v1.27.4"
I1109 08:43:18.997967       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1109 08:43:19.092362       1 config.go:188] "Starting service config controller"
I1109 08:43:19.093505       1 shared_informer.go:311] Waiting for caches to sync for service config
I1109 08:43:19.094156       1 config.go:97] "Starting endpoint slice config controller"
I1109 08:43:19.094191       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1109 08:43:19.325632       1 config.go:315] "Starting node config controller"
I1109 08:43:19.325694       1 shared_informer.go:311] Waiting for caches to sync for node config
I1109 08:43:19.389514       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1109 08:43:19.892563       1 shared_informer.go:318] Caches are synced for node config
I1109 08:43:19.933563       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [41c330632ed9] <==
* I1109 08:43:00.929836       1 serving.go:348] Generated self-signed cert in-memory
W1109 08:43:04.891989       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1109 08:43:04.892051       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1109 08:43:04.892076       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1109 08:43:04.892091       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1109 08:43:05.025958       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I1109 08:43:05.026035       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1109 08:43:05.044692       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1109 08:43:05.089461       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1109 08:43:05.047014       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1109 08:43:05.047048       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1109 08:43:05.197698       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [5ff8aacb1474] <==
* I1109 07:30:15.496151       1 serving.go:348] Generated self-signed cert in-memory
W1109 07:30:20.062960       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1109 07:30:20.063000       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1109 07:30:20.063018       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1109 07:30:20.063031       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1109 07:30:21.360705       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I1109 07:30:21.360753       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1109 07:30:21.402205       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1109 07:30:21.410313       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1109 07:30:21.411484       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1109 07:30:21.411528       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1109 07:30:21.545177       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1109 08:38:41.106098       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1109 08:38:41.108435       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1109 08:38:41.120764       1 scheduling_queue.go:1139] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
I1109 08:38:41.121207       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
E1109 08:38:41.124802       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Nov 09 09:57:35 minikube kubelet[1631]: E1109 09:57:35.842170    1631 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 09 09:57:35 minikube kubelet[1631]: E1109 09:57:35.842581    1631 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/8c98e399-b285-4951-ac2b-2cabe48870fb-webhook-cert podName:8c98e399-b285-4951-ac2b-2cabe48870fb nodeName:}" failed. No retries permitted until 2023-11-09 09:59:37.842492219 +0000 UTC m=+3727.923598708 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/8c98e399-b285-4951-ac2b-2cabe48870fb-webhook-cert") pod "ingress-nginx-controller-7cb7b799bd-b46sn" (UID: "8c98e399-b285-4951-ac2b-2cabe48870fb") : secret "ingress-nginx-admission" not found
Nov 09 09:57:58 minikube kubelet[1631]: E1109 09:57:58.540185    1631 kubelet.go:1875] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7cb7b799bd-b46sn"
Nov 09 09:57:58 minikube kubelet[1631]: E1109 09:57:58.540435    1631 pod_workers.go:1294] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7cb7b799bd-b46sn" podUID=8c98e399-b285-4951-ac2b-2cabe48870fb
Nov 09 09:59:37 minikube kubelet[1631]: E1109 09:59:37.899137    1631 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 09 09:59:37 minikube kubelet[1631]: E1109 09:59:37.900653    1631 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/8c98e399-b285-4951-ac2b-2cabe48870fb-webhook-cert podName:8c98e399-b285-4951-ac2b-2cabe48870fb nodeName:}" failed. No retries permitted until 2023-11-09 10:01:39.900460185 +0000 UTC m=+3849.994057829 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/8c98e399-b285-4951-ac2b-2cabe48870fb-webhook-cert") pod "ingress-nginx-controller-7cb7b799bd-b46sn" (UID: "8c98e399-b285-4951-ac2b-2cabe48870fb") : secret "ingress-nginx-admission" not found
Nov 09 10:00:12 minikube kubelet[1631]: E1109 10:00:12.539422    1631 kubelet.go:1875] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7cb7b799bd-b46sn"
Nov 09 10:00:12 minikube kubelet[1631]: E1109 10:00:12.539685    1631 pod_workers.go:1294] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7cb7b799bd-b46sn" podUID=8c98e399-b285-4951-ac2b-2cabe48870fb
Nov 09 10:00:30 minikube kubelet[1631]: I1109 10:00:30.320668    1631 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-df87n\" (UniqueName: \"kubernetes.io/projected/f2fe50cf-0066-4a8b-8793-8b7d9d117f72-kube-api-access-df87n\") pod \"f2fe50cf-0066-4a8b-8793-8b7d9d117f72\" (UID: \"f2fe50cf-0066-4a8b-8793-8b7d9d117f72\") "
Nov 09 10:00:30 minikube kubelet[1631]: I1109 10:00:30.324114    1631 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f2fe50cf-0066-4a8b-8793-8b7d9d117f72-kube-api-access-df87n" (OuterVolumeSpecName: "kube-api-access-df87n") pod "f2fe50cf-0066-4a8b-8793-8b7d9d117f72" (UID: "f2fe50cf-0066-4a8b-8793-8b7d9d117f72"). InnerVolumeSpecName "kube-api-access-df87n". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 09 10:00:30 minikube kubelet[1631]: I1109 10:00:30.422465    1631 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-df87n\" (UniqueName: \"kubernetes.io/projected/f2fe50cf-0066-4a8b-8793-8b7d9d117f72-kube-api-access-df87n\") on node \"minikube\" DevicePath \"\""
Nov 09 10:00:30 minikube kubelet[1631]: I1109 10:00:30.630342    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d776372db46db0529b4656f7976619cfa15122e9830e93e6a73f11c7ef31ccd8"
Nov 09 10:00:30 minikube kubelet[1631]: E1109 10:00:30.872084    1631 cadvisor_stats_provider.go:442] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods/besteffort/podf2fe50cf-0066-4a8b-8793-8b7d9d117f72/d776372db46db0529b4656f7976619cfa15122e9830e93e6a73f11c7ef31ccd8\": RecentStats: unable to find data in memory cache]"
Nov 09 10:00:31 minikube kubelet[1631]: I1109 10:00:31.444113    1631 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-jzvwd\" (UniqueName: \"kubernetes.io/projected/337a6d7a-cc84-4ea7-851a-a4dd166c0016-kube-api-access-jzvwd\") pod \"337a6d7a-cc84-4ea7-851a-a4dd166c0016\" (UID: \"337a6d7a-cc84-4ea7-851a-a4dd166c0016\") "
Nov 09 10:00:31 minikube kubelet[1631]: I1109 10:00:31.453347    1631 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/337a6d7a-cc84-4ea7-851a-a4dd166c0016-kube-api-access-jzvwd" (OuterVolumeSpecName: "kube-api-access-jzvwd") pod "337a6d7a-cc84-4ea7-851a-a4dd166c0016" (UID: "337a6d7a-cc84-4ea7-851a-a4dd166c0016"). InnerVolumeSpecName "kube-api-access-jzvwd". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 09 10:00:31 minikube kubelet[1631]: I1109 10:00:31.546049    1631 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-jzvwd\" (UniqueName: \"kubernetes.io/projected/337a6d7a-cc84-4ea7-851a-a4dd166c0016-kube-api-access-jzvwd\") on node \"minikube\" DevicePath \"\""
Nov 09 10:00:31 minikube kubelet[1631]: I1109 10:00:31.696004    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d94b43bc75d84307a38bc9db66814f1849a69aeafa451c8c223bf3bfe66b2d7a"
Nov 09 10:01:40 minikube kubelet[1631]: I1109 10:01:40.751004    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="76696029a74bae93b309a1a6845626b4ac9fd7a5161c19379dbaebb47995f55b"
Nov 09 10:02:30 minikube kubelet[1631]: W1109 10:02:30.633261    1631 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 09 10:07:30 minikube kubelet[1631]: W1109 10:07:30.609091    1631 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 09 10:12:30 minikube kubelet[1631]: W1109 10:12:30.620614    1631 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 09 10:13:21 minikube kubelet[1631]: I1109 10:13:21.225780    1631 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-7cb7b799bd-b46sn" podStartSLOduration=622.293809392 podCreationTimestamp="2023-11-09 09:51:23 +0000 UTC" firstStartedPulling="2023-11-09 10:01:40.801256782 +0000 UTC m=+3850.901318019" lastFinishedPulling="2023-11-09 10:13:16.683416444 +0000 UTC m=+4546.787166241" observedRunningTime="2023-11-09 10:13:21.174255543 +0000 UTC m=+4551.278005340" watchObservedRunningTime="2023-11-09 10:13:21.179657614 +0000 UTC m=+4551.283407311"
Nov 09 10:17:30 minikube kubelet[1631]: W1109 10:17:30.628574    1631 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 09 10:22:30 minikube kubelet[1631]: W1109 10:22:30.608513    1631 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 09 10:23:24 minikube kubelet[1631]: I1109 10:23:24.247233    1631 topology_manager.go:212] "Topology Admit Handler"
Nov 09 10:23:24 minikube kubelet[1631]: E1109 10:23:24.247616    1631 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="337a6d7a-cc84-4ea7-851a-a4dd166c0016" containerName="patch"
Nov 09 10:23:24 minikube kubelet[1631]: E1109 10:23:24.247660    1631 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f2fe50cf-0066-4a8b-8793-8b7d9d117f72" containerName="create"
Nov 09 10:23:24 minikube kubelet[1631]: I1109 10:23:24.257001    1631 memory_manager.go:346] "RemoveStaleState removing state" podUID="337a6d7a-cc84-4ea7-851a-a4dd166c0016" containerName="patch"
Nov 09 10:23:24 minikube kubelet[1631]: I1109 10:23:24.257081    1631 memory_manager.go:346] "RemoveStaleState removing state" podUID="f2fe50cf-0066-4a8b-8793-8b7d9d117f72" containerName="create"
Nov 09 10:23:24 minikube kubelet[1631]: I1109 10:23:24.347867    1631 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fflqk\" (UniqueName: \"kubernetes.io/projected/28b4b1e5-e6f6-43dd-8f30-cb5b405de91d-kube-api-access-fflqk\") pod \"ingress-nginx-controller-7799c6795f-lq226\" (UID: \"28b4b1e5-e6f6-43dd-8f30-cb5b405de91d\") " pod="ingress-nginx/ingress-nginx-controller-7799c6795f-lq226"
Nov 09 10:23:24 minikube kubelet[1631]: I1109 10:23:24.348512    1631 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/28b4b1e5-e6f6-43dd-8f30-cb5b405de91d-webhook-cert\") pod \"ingress-nginx-controller-7799c6795f-lq226\" (UID: \"28b4b1e5-e6f6-43dd-8f30-cb5b405de91d\") " pod="ingress-nginx/ingress-nginx-controller-7799c6795f-lq226"
Nov 09 10:23:26 minikube kubelet[1631]: I1109 10:23:26.022689    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="bf9f9391f3832fb6c128a1cbd960061691fe5f47637b6ed929551f184c114f0d"
Nov 09 10:23:26 minikube kubelet[1631]: I1109 10:23:26.928603    1631 topology_manager.go:212] "Topology Admit Handler"
Nov 09 10:23:27 minikube kubelet[1631]: I1109 10:23:27.054662    1631 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-c57w5\" (UniqueName: \"kubernetes.io/projected/797fed9a-dacd-4077-b5cf-e1917cd1807a-kube-api-access-c57w5\") pod \"ingress-nginx-admission-create-2q5n8\" (UID: \"797fed9a-dacd-4077-b5cf-e1917cd1807a\") " pod="ingress-nginx/ingress-nginx-admission-create-2q5n8"
Nov 09 10:23:27 minikube kubelet[1631]: I1109 10:23:27.099271    1631 topology_manager.go:212] "Topology Admit Handler"
Nov 09 10:23:27 minikube kubelet[1631]: I1109 10:23:27.256018    1631 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4vvdj\" (UniqueName: \"kubernetes.io/projected/0be482ce-d21c-4ff1-9ad3-5ebe478f9614-kube-api-access-4vvdj\") pod \"ingress-nginx-admission-patch-95jzt\" (UID: \"0be482ce-d21c-4ff1-9ad3-5ebe478f9614\") " pod="ingress-nginx/ingress-nginx-admission-patch-95jzt"
Nov 09 10:23:28 minikube kubelet[1631]: I1109 10:23:28.543882    1631 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=337a6d7a-cc84-4ea7-851a-a4dd166c0016 path="/var/lib/kubelet/pods/337a6d7a-cc84-4ea7-851a-a4dd166c0016/volumes"
Nov 09 10:23:28 minikube kubelet[1631]: I1109 10:23:28.547427    1631 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=f2fe50cf-0066-4a8b-8793-8b7d9d117f72 path="/var/lib/kubelet/pods/f2fe50cf-0066-4a8b-8793-8b7d9d117f72/volumes"
Nov 09 10:23:31 minikube kubelet[1631]: I1109 10:23:31.820274    1631 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-4vvdj\" (UniqueName: \"kubernetes.io/projected/0be482ce-d21c-4ff1-9ad3-5ebe478f9614-kube-api-access-4vvdj\") pod \"0be482ce-d21c-4ff1-9ad3-5ebe478f9614\" (UID: \"0be482ce-d21c-4ff1-9ad3-5ebe478f9614\") "
Nov 09 10:23:31 minikube kubelet[1631]: I1109 10:23:31.901560    1631 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/0be482ce-d21c-4ff1-9ad3-5ebe478f9614-kube-api-access-4vvdj" (OuterVolumeSpecName: "kube-api-access-4vvdj") pod "0be482ce-d21c-4ff1-9ad3-5ebe478f9614" (UID: "0be482ce-d21c-4ff1-9ad3-5ebe478f9614"). InnerVolumeSpecName "kube-api-access-4vvdj". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 09 10:23:31 minikube kubelet[1631]: I1109 10:23:31.920822    1631 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-4vvdj\" (UniqueName: \"kubernetes.io/projected/0be482ce-d21c-4ff1-9ad3-5ebe478f9614-kube-api-access-4vvdj\") on node \"minikube\" DevicePath \"\""
Nov 09 10:23:32 minikube kubelet[1631]: I1109 10:23:32.038431    1631 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-c57w5\" (UniqueName: \"kubernetes.io/projected/797fed9a-dacd-4077-b5cf-e1917cd1807a-kube-api-access-c57w5\") pod \"797fed9a-dacd-4077-b5cf-e1917cd1807a\" (UID: \"797fed9a-dacd-4077-b5cf-e1917cd1807a\") "
Nov 09 10:23:32 minikube kubelet[1631]: I1109 10:23:32.076632    1631 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/797fed9a-dacd-4077-b5cf-e1917cd1807a-kube-api-access-c57w5" (OuterVolumeSpecName: "kube-api-access-c57w5") pod "797fed9a-dacd-4077-b5cf-e1917cd1807a" (UID: "797fed9a-dacd-4077-b5cf-e1917cd1807a"). InnerVolumeSpecName "kube-api-access-c57w5". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 09 10:23:32 minikube kubelet[1631]: I1109 10:23:32.139058    1631 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-c57w5\" (UniqueName: \"kubernetes.io/projected/797fed9a-dacd-4077-b5cf-e1917cd1807a-kube-api-access-c57w5\") on node \"minikube\" DevicePath \"\""
Nov 09 10:23:32 minikube kubelet[1631]: I1109 10:23:32.599827    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ef746b891c0688e3434163ce46dd1eb5226b60d846e5ba32528c8ad4f025a5f5"
Nov 09 10:23:32 minikube kubelet[1631]: I1109 10:23:32.625191    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a6f8a4b4a2ffd3f2e57f56288fa4ab6a45129ef8b017ed09fd4f1f2a99925c84"
Nov 09 10:23:36 minikube kubelet[1631]: I1109 10:23:36.812520    1631 scope.go:115] "RemoveContainer" containerID="a89cc314263cdf830a36b66f941017202828aaeaf36c26f850615b69a75463a6"
Nov 09 10:23:36 minikube kubelet[1631]: I1109 10:23:36.857803    1631 scope.go:115] "RemoveContainer" containerID="0b2afffebb723f0b1b7fe580a8217ef8417a647218e46f11eb6aed8560463e01"
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.193786    1631 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/8c98e399-b285-4951-ac2b-2cabe48870fb-webhook-cert\") pod \"8c98e399-b285-4951-ac2b-2cabe48870fb\" (UID: \"8c98e399-b285-4951-ac2b-2cabe48870fb\") "
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.193967    1631 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-nwl8l\" (UniqueName: \"kubernetes.io/projected/8c98e399-b285-4951-ac2b-2cabe48870fb-kube-api-access-nwl8l\") pod \"8c98e399-b285-4951-ac2b-2cabe48870fb\" (UID: \"8c98e399-b285-4951-ac2b-2cabe48870fb\") "
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.197440    1631 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8c98e399-b285-4951-ac2b-2cabe48870fb-kube-api-access-nwl8l" (OuterVolumeSpecName: "kube-api-access-nwl8l") pod "8c98e399-b285-4951-ac2b-2cabe48870fb" (UID: "8c98e399-b285-4951-ac2b-2cabe48870fb"). InnerVolumeSpecName "kube-api-access-nwl8l". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.197792    1631 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/8c98e399-b285-4951-ac2b-2cabe48870fb-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "8c98e399-b285-4951-ac2b-2cabe48870fb" (UID: "8c98e399-b285-4951-ac2b-2cabe48870fb"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.295187    1631 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-nwl8l\" (UniqueName: \"kubernetes.io/projected/8c98e399-b285-4951-ac2b-2cabe48870fb-kube-api-access-nwl8l\") on node \"minikube\" DevicePath \"\""
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.295311    1631 reconciler_common.go:300] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/8c98e399-b285-4951-ac2b-2cabe48870fb-webhook-cert\") on node \"minikube\" DevicePath \"\""
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.768919    1631 scope.go:115] "RemoveContainer" containerID="0a6c1e34bc19284c3f33a4333dd1a76d14c073c2a24c38b22d19d6037e1b89da"
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.811256    1631 scope.go:115] "RemoveContainer" containerID="0a6c1e34bc19284c3f33a4333dd1a76d14c073c2a24c38b22d19d6037e1b89da"
Nov 09 10:23:37 minikube kubelet[1631]: E1109 10:23:37.817263    1631 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 0a6c1e34bc19284c3f33a4333dd1a76d14c073c2a24c38b22d19d6037e1b89da" containerID="0a6c1e34bc19284c3f33a4333dd1a76d14c073c2a24c38b22d19d6037e1b89da"
Nov 09 10:23:37 minikube kubelet[1631]: I1109 10:23:37.817694    1631 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:0a6c1e34bc19284c3f33a4333dd1a76d14c073c2a24c38b22d19d6037e1b89da} err="failed to get container status \"0a6c1e34bc19284c3f33a4333dd1a76d14c073c2a24c38b22d19d6037e1b89da\": rpc error: code = Unknown desc = Error response from daemon: No such container: 0a6c1e34bc19284c3f33a4333dd1a76d14c073c2a24c38b22d19d6037e1b89da"
Nov 09 10:23:38 minikube kubelet[1631]: I1109 10:23:38.529429    1631 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=8c98e399-b285-4951-ac2b-2cabe48870fb path="/var/lib/kubelet/pods/8c98e399-b285-4951-ac2b-2cabe48870fb/volumes"
Nov 09 10:27:30 minikube kubelet[1631]: W1109 10:27:30.602957    1631 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> kubernetes-dashboard [365b39d70a5f] <==
* 2023/11/09 08:43:22 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": net/http: TLS handshake timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00055fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc0001da080)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2023/11/09 08:43:22 Using namespace: kubernetes-dashboard
2023/11/09 08:43:22 Using in-cluster config to connect to apiserver
2023/11/09 08:43:22 Using secret token for csrf signing
2023/11/09 08:43:22 Initializing csrf token from kubernetes-dashboard-csrf secret

* 
* ==> kubernetes-dashboard [abbb1aacb570] <==
* 2023/11/09 08:43:49 Starting overwatch
2023/11/09 08:43:49 Using namespace: kubernetes-dashboard
2023/11/09 08:43:49 Using in-cluster config to connect to apiserver
2023/11/09 08:43:49 Using secret token for csrf signing
2023/11/09 08:43:49 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/11/09 08:43:49 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/11/09 08:43:49 Successful initial request to the apiserver, version: v1.27.4
2023/11/09 08:43:49 Generating JWE encryption key
2023/11/09 08:43:49 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/11/09 08:43:49 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/11/09 08:43:49 Initializing JWE encryption key from synchronized object
2023/11/09 08:43:49 Creating in-cluster Sidecar client
2023/11/09 08:43:49 Serving insecurely on HTTP port: 9090
2023/11/09 08:43:49 Successful request to sidecar

* 
* ==> storage-provisioner [29ca89f09296] <==
* I1109 08:43:22.002890       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1109 08:43:32.253598       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

* 
* ==> storage-provisioner [f28846e9c2eb] <==
* I1109 08:43:46.617239       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1109 08:43:46.654761       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1109 08:43:46.656125       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1109 08:44:04.107086       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1109 08:44:04.109058       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"6e3ae8b9-fc60-4375-85c1-1ee169f31cb1", APIVersion:"v1", ResourceVersion:"23981", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_38eaf854-e27f-4391-99ea-83ccda4b4cf6 became leader
I1109 08:44:04.110899       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_38eaf854-e27f-4391-99ea-83ccda4b4cf6!
I1109 08:44:04.212008       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_38eaf854-e27f-4391-99ea-83ccda4b4cf6!

